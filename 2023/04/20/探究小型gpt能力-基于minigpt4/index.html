


<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>  探究小型Gpt能力-基于Minigpt4 |    Yiyang Ai Lab.</title>
  <meta name="description" content="Personal website. Recording some creative things">
  <!-- 标签页图标 -->
  
  <link rel="shortcut icon" href="https://github.com/YiyangZhou/imageBeds/blob/main/imgs/collage-line.png" type="image/x-icon">
  

  <!-- 图标库 -->
  <link href="https://cdn.jsdelivr.net/npm/remixicon@2.2.0/fonts/remixicon.css" rel="stylesheet">
  <!-- 动画库 -->
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fushaolei/cdn-white@1.0/css/animate.css"/>
  
  <!-- css文件 -->
  
<link rel="stylesheet" href="/css/white.css">

  <!-- 代码高亮 -->
  
<meta name="generator" content="Hexo 5.4.2"></head>


<body>

<div class="menu-outer">
    <div class="menu-inner">
      <div class="menu-site-name  animate__animated  animate__fadeInUp">
        <a href="/">
          Yiyang Ai Lab.
        </a>
        
      </div>
      <div class="menu-group">
        <ul class="menu-ul">
        
          <a href="/" class="nav-link">
            <li class="menu-li  animate__animated  animate__fadeInUp">
              HOME
            </li>
          </a>
        
          <a href="/archives" class="nav-link">
            <li class="menu-li  animate__animated  animate__fadeInUp">
              BLOG
            </li>
          </a>
        
        
        
          <li class="menu-li animate__animated  animate__fadeInUp" id="mobile-menu">
            <i class="ri-menu-line"></i>
          </li>
        
        </ul>

      </div>

    </div>
</div>
<div id="mobile-main" class="animate__animated  animate__fadeIn">
  <div class="mobile-menu-inner">
    <div class="mobile-menu-site-name animate__animated  animate__fadeInUp">
      <a href="/">
        Yiyang Ai Lab.
      </a>
    </div>
    <div class="mobile-menu-group" id="mobile-close">
      <i class="ri-close-line"></i>
    </div>

  </div>

  <div class="mobile-menu-div">
  
    <a href="/" class="mobile-nav-link">
      <div class="mobile-menu-child animate__animated  animate__fadeInUp">
        <span>HOME</span>
      </div>
    </a>
  
    <a href="/archives" class="mobile-nav-link">
      <div class="mobile-menu-child animate__animated  animate__fadeInUp">
        <span>BLOG</span>
      </div>
    </a>
  
  
  </div>


</div>

<div class="body-outer">
  <div class="body-inner">
    
<article class="post-inner">
  <div class="post-content-outer">
    <div class="post-intro">
      <div class="post-title animate__animated  animate__fadeInUp">探究小型gpt能力-基于minigpt4</div>
      <div class="meta-intro animate__animated  animate__fadeInUp">Apr 20 2023</div>
      
    </div>
    <div class="post-content-inner">
      <div class="post-content-inner-space">

      </div>
      <div class="post-content-main animate__animated  animate__fadeInUp">
        <!-- top型目录 -->
        
        <p>在开始之前先学习一下低成本微调大模型的方案：<strong>LoRA</strong>：<a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/2106.09685.pdf">LoRA: Low-Rank Adaptation of Large Language Models</a></p>
<h1 id="1-LoRA的原理"><a href="#1-LoRA的原理" class="headerlink" title="1.LoRA的原理"></a>1.LoRA的原理</h1><p>LoRA是一种以极低资源微调大模型的方法。</p>
<h2 id="1-1大模型微调的困境"><a href="#1-1大模型微调的困境" class="headerlink" title="1.1大模型微调的困境"></a>1.1大模型微调的困境</h2><p>随着模型规模的不断扩大，模型会”涌现”出各种能力。特别是对大语言模型(LLM)来说，随着规模的扩大其在zero-shot、常识推理等能力上会有大幅度的提高。相比于规模较小的模型，大模型的微调成本和部署成本都非常高。例如，GPT-3 175B模型微调需要1.2TB的显存。此外，若针对不同下游任务微调多个模型，那么就需要为每个下游任务保存一份模型权重，成本非常高。<strong>在某些场景下，甚至可能需要针对不同的用户微调不同的模型，那么模型微调和部署的成本将不可接受</strong>。</p>
<h2 id="1-2LoRA之前的方法"><a href="#1-2LoRA之前的方法" class="headerlink" title="1.2LoRA之前的方法"></a>1.2LoRA之前的方法</h2><p>在LoRA方法提出之前，也有很多方法尝试解决大模型微调困境的方法。其中有两个主要的方向：(1) 添加adapter层；(2) 由于某种形式的输入层激活。但是这两种方法都有局限性：</p>
<p>（1）Adapter：</p>
<p>缺点：Adapter层会引入推理时延</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230428145730956.png" >
        </sapn>
      </p>
<p>简单来说，adapter就是固定原有的参数，并添加一些额外参数用于微调。上图中会在原始的transformer block中添加2个adapter，一个在多头注意力后面，另一个这是FFN后面。</p>
<p>显然，adapter会在模型中添加额外的层，这些层会导致大模型在推理时需要更多的GPU通信，而且也会约束模型并行。<strong>这些问题都将导致模型推理变慢</strong>。</p>
<p>（2）prefix-tuning：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230428150834618.png" >
        </sapn>
      </p>
<p>prefix-tuning方法是受语言模型in-context learning能力的启发，只要有合适的上下文则语言模型可以很好的解决自然语言任务。但是，针对特定的任务找到离散token的前缀需要花费很长时间，prefix-tuning提出使用连续的virtual token embedding来替换离散token。</p>
<p>具体来说，对于transformer中的每一层，都在句子表征前面插入可训练的virtual token embedding。对于自回归模型(GPT系列)，在句子前添加连续前缀，即 z=[PREFIX;x;y] 。对于Encoder-Decoder模型(T5)，则在Ecoder和Decoder前都添加连续前缀 z=[PREFIX;x|PREFIX′;y] 。<strong>添加前缀的过程如上图所示</strong>。</p>
<p><strong>虽然，prefix-tuning并没有添加太多的额外参数。但是，prefix-tuning难以优化，且会减少下游任务的序列长度。</strong></p>
<h2 id="1-3问题的正式表述"><a href="#1-3问题的正式表述" class="headerlink" title="1.3问题的正式表述"></a>1.3问题的正式表述</h2><p><strong>术语与约定</strong>。由于LoRA原理的介绍，会使用Transformer架构。因此，这里先给出一些术语约定。一个Transformer层的输入和输出维度尺寸为：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230428151054073.png" >
        </sapn>
      </p>
<p>，使用 W<sub>q</sub>、W<sub>k</sub>、W<sub>v</sub>和W<sub>o</sub>表示自注意力模块中的query/key/value/output投影矩阵。 W或W<sub>0</sub> 表示预训练模型的权重矩阵， ΔW 表示模型在适配过程中的梯度更新。r来表示LoRA模块的秩。使用Adam作为模型优化器，Transformer MLP前馈层的维度为 :</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230428151432190.png" >
        </sapn>
      。</p>
<p><strong>问题表述</strong>。LoRA虽然与训练目标无关，这里还是以语言建模为例。假设给定一个预训练的自回归语言模型 P<sub>Φ</sub>sub&gt;(y|x) , Φ 是模型参数。目标是使该语言模型适应下游的摘要、机器阅读理解等任务。每个下游任务都有context-target样本对组成的训练集： z={(x<sub>i</sub>,y<sub>i</sub>)}<sub>i=1,…,N</sub>，其中 x<sub>i</sub> 和 y<sub>i</sub> 都是token序列。例如，对于摘要任务， x<sub>i</sub>  是文章内容，y<sub>i</sub>是摘要。</p>
<p>在完整微调的过程中，模型使用预训练好的权重 Φ<sub>0</sub> 来初始化模型，然后通过最大化条件语言模型来更新参数 Φ<sub>0</sub>+ΔΦ ：<br>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230428151954481.png" >
        </sapn>
      </p>
<p>完整微调的主要缺点：对于每个下游任务，都需要学习不同的参数更新 ΔΦ ，其中维度 |ΔΦ|=| Φ<sub>0</sub> | 。因此，如果预训练模型很大，存储和部署许多独立的微调模型实例非常有挑战。</p>
<p>LoRA为了更加的参数高效，使用相对非常小的参数 Θ 来表示任务相关的参数增量 ΔΦ=ΔΦ(Θ) ，其中 |Θ|≪| Φ<sub>0</sub> | 。寻找 ΔΦ 的任务就变成对 Θ 的优化:<br>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230428152049131.png" >
        </sapn>
      </p>
<p>LoRA将会使用低秩表示来编码 ΔΦ ，同时实现计算高效和存储高效。当预训练模型是175B GPT-3，可训练参数 |Θ| 可以小至 |Φ<sub>0</sub> | 的 0.01% 。</p>
<h2 id="1-4LoRA"><a href="#1-4LoRA" class="headerlink" title="1.4LoRA"></a>1.4LoRA</h2><p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230428153415914.png" >
        </sapn>
      </p>
<p>通常，神经网络中会包含许多进行矩阵乘法的稠密层，这些层通常是满秩的。在模型适配下游任务的过程中，权重更新也应该具有低的“内在秩”。对于预训练权重矩阵 W<sub>0</sub>∈R<sup>d×k</sup> ，可以通过低秩分解来表示其更新 ，W<sub>0</sub>+ΔW=W<sub>0</sub>+BA，B∈R<sup>d×r</sup>, A∈R<sup>r×k</sup> 且秩 r≪min(d,k) 。在训练过程中， W<sub>0</sub>被冻结且不接受梯度更新，A和B则是可训练参数。注意， W<sub>0</sub>和 ΔW=BA 都会乘以相同的输入。对于h=W<sub>0</sub>x ，前向传播变为：<br>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230428161958237.png" >
        </sapn>
      </p>
<p>对矩阵 A 使用随机高斯初始化，对矩阵 B 使用0进行初始化，因此 ΔW=BA 在训练的开始为0。使用 a/r 来缩放 ΔWx 。当使用Adam优化时，经过适当的缩放初始化，调优a与调优学习率大致相同。</p>
<p>当进行部署时，以显式的计算和存储 W=W<sub>0</sub>+BA ，并正常执行推理。 W<sub>0</sub> 和BA都是R<sup>d×k</sup>。当需要转换至另一个下游任务，可以通过减去 BA来恢复W<sub>0</sub>  ，然后添加不同的 B′A′ 。至关重要的是，这保证不会引人任何额外的推理时延。</p>

        <!-- 分类文章 -->
        
      </div>
      <div class="post-content-inner-space">
        
          <div class="space-toc-main animate__animated  animate__fadeInUp">
            <ol class="space-toc"><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#1-1%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E7%9A%84%E5%9B%B0%E5%A2%83"><span class="space-toc-text">1.1大模型微调的困境</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#1-2LoRA%E4%B9%8B%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95"><span class="space-toc-text">1.2LoRA之前的方法</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#1-3%E9%97%AE%E9%A2%98%E7%9A%84%E6%AD%A3%E5%BC%8F%E8%A1%A8%E8%BF%B0"><span class="space-toc-text">1.3问题的正式表述</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#1-4LoRA"><span class="space-toc-text">1.4LoRA</span></a></li></ol>
           </div>
        
      </div>
   </div>
    <!-- 评论 -->
    
  </div>
</article>
  </div>
</div>



<!-- 如果是home模式的话，不在首页就显示footer，如果不是home模式的话 所有都显示footer -->

  <div class="footer-outer animate__animated  animate__fadeInUp">
    <div class="footer-inner">
    <div class="footer-text">
    <p>Power by <a target="_blank" rel="noopener" href="http://hexo.io/">Hexo</a> Theme by <a target="_blank" rel="noopener" href="https://github.com/FuShaoLei/hexo-theme-white">White</a></p>

    </div>
    <div class="footer-contact">
    <ul class="footer-ul">
        
        <li class="footer-li">
            <a href="https://github.com/YiyangZhou" target="_blank">
                <i class="ri-github-line"></i>
            </a>
        </li>
        
        <li class="footer-li">
            <a href="mailto:zhouyiyangailab@gmail.com" target="_blank">
                <i class="ri-mail-line"></i>
            </a>
        </li>
        
        <li class="footer-li">
            <a href="https://scholar.google.com/citations?user=6KltFMAAAAAJ&amp;hl=zh-CN" target="_blank">
                <i class="ri-google-fill"></i>
            </a>
        </li>
        
    </ul>
    </div>
    </div>
</div>






<script src="/js/white.js"></script>



</body>
</html>
