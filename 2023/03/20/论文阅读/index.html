


<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>  论文阅读 |    Yiyang Ai Lab.</title>
  <meta name="description" content="Personal website. Recording some creative things">
  <!-- 标签页图标 -->
  
  <link rel="shortcut icon" href="https://github.com/YiyangZhou/imageBeds/blob/main/imgs/collage-line.png" type="image/x-icon">
  

  <!-- 图标库 -->
  <link href="https://cdn.jsdelivr.net/npm/remixicon@2.2.0/fonts/remixicon.css" rel="stylesheet">
  <!-- 动画库 -->
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fushaolei/cdn-white@1.0/css/animate.css"/>
  
  <!-- css文件 -->
  
<link rel="stylesheet" href="/css/white.css">

  <!-- 代码高亮 -->
  
<meta name="generator" content="Hexo 5.4.2"></head>


<body>

<div class="menu-outer">
    <div class="menu-inner">
      <div class="menu-site-name  animate__animated  animate__fadeInUp">
        <a href="/">
          Yiyang Ai Lab.
        </a>
        
      </div>
      <div class="menu-group">
        <ul class="menu-ul">
        
          <a href="/" class="nav-link">
            <li class="menu-li  animate__animated  animate__fadeInUp">
              HOME
            </li>
          </a>
        
          <a href="/archives" class="nav-link">
            <li class="menu-li  animate__animated  animate__fadeInUp">
              BLOG
            </li>
          </a>
        
        
        
          <li class="menu-li animate__animated  animate__fadeInUp" id="mobile-menu">
            <i class="ri-menu-line"></i>
          </li>
        
        </ul>

      </div>

    </div>
</div>
<div id="mobile-main" class="animate__animated  animate__fadeIn">
  <div class="mobile-menu-inner">
    <div class="mobile-menu-site-name animate__animated  animate__fadeInUp">
      <a href="/">
        Yiyang Ai Lab.
      </a>
    </div>
    <div class="mobile-menu-group" id="mobile-close">
      <i class="ri-close-line"></i>
    </div>

  </div>

  <div class="mobile-menu-div">
  
    <a href="/" class="mobile-nav-link">
      <div class="mobile-menu-child animate__animated  animate__fadeInUp">
        <span>HOME</span>
      </div>
    </a>
  
    <a href="/archives" class="mobile-nav-link">
      <div class="mobile-menu-child animate__animated  animate__fadeInUp">
        <span>BLOG</span>
      </div>
    </a>
  
  
  </div>


</div>

<div class="body-outer">
  <div class="body-inner">
    
<article class="post-inner">
  <div class="post-content-outer">
    <div class="post-intro">
      <div class="post-title animate__animated  animate__fadeInUp">论文阅读</div>
      <div class="meta-intro animate__animated  animate__fadeInUp">Mar 20 2023</div>
      
    </div>
    <div class="post-content-inner">
      <div class="post-content-inner-space">

      </div>
      <div class="post-content-main animate__animated  animate__fadeInUp">
        <!-- top型目录 -->
        
        <h1 id="ChatGPT相关"><a href="#ChatGPT相关" class="headerlink" title="ChatGPT相关"></a>ChatGPT相关</h1><h2 id="1-相关文献总结"><a href="#1-相关文献总结" class="headerlink" title="1.相关文献总结"></a>1.相关文献总结</h2><p><strong>（1）多模态预训练：</strong></p>
<p><strong>综述</strong>：</p>
<p>​        Vision-Language Pre-training: Basics, Recent Advances, and Future Trends</p>
<p><strong>多模态框架（包含：ALBEF, BLIP, BLIP2等)</strong>: <a target="_blank" rel="noopener" href="https://github.com/salesforce/LAVIS">https://github.com/salesforce/LAVIS</a>  </p>
<p><strong>博文推荐</strong>：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/616351346">https://zhuanlan.zhihu.com/p/616351346</a></p>
<hr>
<p><strong>（2）数据集：</strong></p>
<p><strong>图文</strong>: </p>
<p>​        MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation (MMDialog) [✅]</p>
<p><strong>视频</strong></p>
<p>​        TikTalk: A Multi-Modal Dialogue Dataset for Real-World Chitchat (TikTalk) [✅]</p>
<hr>
<p><strong>（3）纯文本ChatGPT：</strong></p>
<p><strong>大模型</strong></p>
<p>​        Improving Language Understanding By Genertative Pre-trying (GPT-1) [✅]</p>
<p>​        Language Models are Unsupervised Multitask Learners (GPT-2) [✅]</p>
<p>​        Language Models are Few-Shot Learners (GPT-3) [✅]</p>
<p>​        Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</p>
<p><strong>Instruction tuning</strong></p>
<p>​        Finetuned Language Models Are Zero-Shot Learners (FLAN) [✅]</p>
<p>​        Multitask Prompted Training Enables Zero-Shot Task Generalization (T0) [✅]</p>
<p>​        Scaling Instruction-Finetuned Language Models (Flan-PaLM) [✅]</p>
<p>​        SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions (SELF-INSTRUCT) [✅]</p>
<p>​        The False Promise of Imitating Proprietary LLMs [✅]</p>
<p><strong>大模型+对话</strong></p>
<p>​        Training language models to follow instructions with human feedback (InstructGPT) [✅]</p>
<p>​        Baize: An Open-Source Chat Model with Parameter-Effificient Tuning on Self-Chat Data (Baize) [✅]</p>
<hr>
<p><strong>（4）多模态ChatGPT：</strong></p>
<p>​        Flamingo: a Visual Language Model for Few-Shot Learning (Flamingo) [✅]</p>
<p>​        BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (Blip-2) [✅]</p>
<p>​        GIT: A Generative Image-to-text Transformer for Vision and Language</p>
<p>​        Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering (Prophet) [✅]</p>
<p>​        Language Is Not All You Need: Aligning Perception with Language Models (KOSMOS-1) [✅]</p>
<p>​        Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models (Visual ChatGPT) [✅]</p>
<p>​        MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action (MM-REACT) [✅]</p>
<p>​        REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering (REVIVE) [✅]</p>
<p>​        An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA (PICa) [✅]</p>
<p>​        MULTIINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning (MultiInstruct) [✅]</p>
<p>​        MiniGPT-4:Enhancing Vision-Language Understanding with Advanced Large Language Models (MiniGPT-4) [✅]</p>
<p>​        Visual Instruction Tuning (LLaVa) [✅]</p>
<p>​        Musketeer (All for One, and One for All): A Generalist Vision-Language Model with Task Explanation Prompts [✅]</p>
<p>​        VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks [✅]</p>
<p>​        EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought [✅]</p>
<p>​        Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models [✅]</p>
<hr>
<p><strong>（5）评测：</strong></p>
<p>​        TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering (TIFA) [✅]</p>
<p>​        PandaLM [✅]</p>
<p>​        How Language Model Hallucinations Can Snowball [✅]</p>
<p>​        Evaluating Object Hallucination in Large Vision-Language Models [✅]    </p>
<p>​        Visual Programming for Text-to-Image Generation and Evaluation [✅]</p>
<hr>
<p><strong>（6）幻觉相关工作：</strong></p>
<p>​        ALIGNSCORE: Evaluating Factual Consistenc with A Unified Alignment Function [✅]</p>
<p>​        Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Theory of Mind [✅]</p>
<p>​        Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training [✅]</p>
<p>​        Evaluating Object Hallucination in Large Vision-Language Models [✅]</p>
<p>​        Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback [✅]</p>
<p>​        Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training [✅]</p>
<p>​        Certified Reasoning with Language Models [✅]</p>
<p>​        Exposing and Mitigating Spurious Correlations for Cross-Modal Retrieval [✅]</p>
<p>​        Trusting Your Evidence:Hallucinate Less with Context-aware Decoding [✅]</p>
<p>​        How Language Model Hallucinations Can Snowball [✅]</p>
<p>​        Inference-Time Intervention: Eliciting Truthful Answers from a Language Model [✅]</p>
<h2 id="2-文献阅读笔记"><a href="#2-文献阅读笔记" class="headerlink" title="2.文献阅读笔记"></a>2.文献阅读笔记</h2><h3 id="2-1-Prophet"><a href="#2-1-Prophet" class="headerlink" title="2.1.Prophet"></a>2.1.Prophet</h3><blockquote>
<p>本文指出早期用外部知识库来引导回答图片问题KB-VQA任务，外部知识的一些无关信息可能会造成误导。本文提出了用VQA模型生成一系列针对图片的指导信息，然后用这些构造成prompt去指导文本GPT生成答案。</p>
</blockquote>
<p><strong>Introduction</strong></p>
<p>这部分指出传统的KB-VQA任务会存在两个问题：</p>
<p>（1）所需的知识可能无法从kb中成功检索</p>
<p>（2）即使检索到所需的知识，也不可避免地引入大量无关的知识</p>
<p>但是也会有像PICa这类模型用caption model去获取图片的caption，然后用得到的caption和一些固定格式的promt输入GPT-3去得到答案。</p>
<blockquote>
<p>本文说上述两种方法都有缺陷：</p>
<p>（1）PICa的方法用的caption model可能并不会得到关于问题的信息：</p>
<p>eg.</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230320170146043.png" >
        </sapn>
      </p>
<p>这张图中caption并不会注意到椰子树，如果用这个caption组合，然后问：图中的树会长什么果子？这样会有问题。</p>
<p>（2）在给到一个任务给GPT-3时候，它依赖上下文，所以上下文例子选择至关重要。</p>
</blockquote>
<p>Prophet的上下文策略文中叫做answer heuristics，有两种形式：1.answer candidates. 2.answer-aware examples.</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230320171429176.png" >
        </sapn>
      </p>
<p><strong>Framework</strong></p>
<p>整个模型是两阶段</p>
<p>（1）第一阶段：训练一个VQA模型获得两种形式的answer heuristics</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230324163841914.png" >
        </sapn>
      </p>
<p>​        其中answer-aware examples的选择是通过上述式子计算出融合模态表征同其他样本融合表征的相似度，因为融合后的表征相似度高，他们所在的潜在答案空间重叠就高，然后选取相似度最高的N个做为上下文answer-aware examples。</p>
<p>（2）第二阶段：在启发式增强的提示阶段，answer heuristics和caption被集成到一个格式化的提示中，以指导GPT-3预测一个答案</p>
<p>整个模型如下：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230320172053320.png" >
        </sapn>
      </p>
<h3 id="2-2-MM-React"><a href="#2-2-MM-React" class="headerlink" title="2.2.MM-React"></a>2.2.MM-React</h3><blockquote>
<p>MM-React的prompt设计利于LLM接受多模态信息。本文注重于利用单独的视觉模型和LLM分离的让LLM理解图像。</p>
</blockquote>
<p><strong>Introduction</strong></p>
<p>这部分指出：</p>
<p>​    很多工作是多模态融合，类似于Flamingo和PaLM-E等，这种模型效果好但计算贵。</p>
<p>MM-React输入图片等非文本模态的数据的使用直接采用PATH，相当于告诉语言模型这有个占位符。</p>
<p><strong>Framework</strong></p>
<p>不同的视觉模型提供不同的信息，例如：目标检测，OCR，实体命名模型。MM-React的目的是根据用户以自然语言查询提供的需求来自动化这个过程。</p>
<p>整个框架如下：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230327110743308.png" >
        </sapn>
      </p>
<p>gpt在系统中的一个思考流程：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230327112631996.png" >
        </sapn>
      </p>
<h3 id="2-3-Self-Instruct"><a href="#2-3-Self-Instruct" class="headerlink" title="2.3.Self-Instruct"></a>2.3.Self-Instruct</h3><blockquote>
<p>现在的instruction-tuned llm能力不错，但是人工标注费时费力且质量多样性不高，所以本文提出self-instruct，让模型自己指导自己遵循指令。</p>
</blockquote>
<p><strong>Introduction</strong></p>
<p>人工撰写instruct数据集缺乏多样性，偏向于热门的NLP任务，没有涵盖真正的各种任务和不同的方式来描述它们。考虑到这些限制，要继续提高指令调优模型的质量，就需要进行开发替代方法。</p>
<p>self-instruct的整个流程是一个自我提高算法，从一个人工的instruction小数据集开始，指导自己整个生成过程。在第一阶段中，将提示模型为新任务生成指令。这一步利用现有的指令集合来创建更广泛的指令来定义( 通常是新的)任务。给定新生成的指令集，该框架还为它们创建输入-输出实例，稍后可以用于监督指令调优。最后，用各种措施来删除低质量和重复的指令。这个过程可以重复进行许多交互，直到完成大量的任务。</p>
<p><strong>Framework</strong></p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230330104229062.png" >
        </sapn>
      </p>
<p><strong>Limitation</strong></p>
<p>(1) Tail Phenomena</p>
<p>(2) Dependence on large models</p>
<p>(3) Reinforcing LM biases</p>
<h3 id="2-4-REVIVE"><a href="#2-4-REVIVE" class="headerlink" title="2.4.REVIVE"></a>2.4.REVIVE</h3><blockquote>
<p>本文提出现有的很多kbVQA方法存在两个问题（1）关注整个视觉，或者滑动窗口检索，忽视了region对于vqa任务的重要性（2）没有很好利用视觉模态特征。本文提出的REVIVE说明了这些region信息的重要性，主要利用显示的检索信息和region信息。</p>
</blockquote>
<p><strong>Introduction</strong></p>
<p>现有工作聚焦于整合外部knowledge，本文侧重于视觉一侧聚焦region对于vqa任务的重要性。</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230330142210728.png" >
        </sapn>
      </p>
<p><strong>Framework</strong></p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230330142645321.png" >
        </sapn>
      </p>
<p>整体来说首先搜集了四种信息：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230330144541446.png" >
        </sapn>
      </p>
<p>region视觉信息，box信息描述方位，和box的tag信息，以及caption用于描述物体的关系（感觉很有道理这个caption）。</p>
<p>除了正常的kb知识库Q,本文为region区域也建立了知识库，具体做法为：</p>
<p>（1）reformat原始的知识库，构建成实体和描述</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230330145745769.png" >
        </sapn>
      </p>
<p>（2）检索方式类似于正常的kb知识库</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230330145902576.png" >
        </sapn>
      </p>
<h3 id="2-5-MultiInstruct"><a href="#2-5-MultiInstruct" class="headerlink" title="2.5.MultiInstruct"></a>2.5.MultiInstruct</h3><blockquote>
<p>在这项工作中，作者介绍了多指令，第一个由多模态指令调整基准数据集组成的多模态指令调整基准数据集，模式任务涵盖11个大类别。每个任务至少设计了5000个来自现有开源数据集的实例（输入输出对）和5个专家编写的指令。文中还设计了一个新的评估度量-敏感度，以评估模型对各种指令的敏感性。</p>
</blockquote>
<p><strong>Introduction</strong></p>
<p>每条实例被组织成统一的序列到序列格式，其中输入文本、图像、指令和边界框在相同的标记空间中表示。</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230403100351129.png" >
        </sapn>
      </p>
<p><strong>MultiInstruct</strong></p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230403101401642.png" >
        </sapn>
      </p>
<h3 id="2-6-MiniGPT-4"><a href="#2-6-MiniGPT-4" class="headerlink" title="2.6.MiniGPT-4"></a>2.6.MiniGPT-4</h3><blockquote>
<p>本文认为gpt4得益于使用了更先进的语言模型，本文提出了MiniGPT-4，它使用一个投影层将一个冻结的视觉编码器与一个冻结的LLM（Vicuna）对齐。整个模型只用了大约500万对图像-文本对和额外的3500个精选的高质量对进行投影层的训练。</p>
</blockquote>
<p><strong>Introduction</strong></p>
<p>本文主要认为gpt4的超强能力都源于一个强大的LLM，文本端是Vicuna视觉端是ViT-G/14和Q-Former。MiniGPT-4只添加了一个投影层，以将编码的视觉特征与Vicuna语言模型对齐，并冻结了所有其他的视觉和语言组件。</p>
<p>MiniGPT-4先用基本的图文对数据来train，使得文本图像端能对齐，然后用3500高质量pairs+自定模版来激发能力。</p>
<p>本文证明了：</p>
<p>​    （1）我们的研究表明，通过将视觉特征与先进的大型语言模型Vicuna相结合，我们可以实现突发的视觉语言能力。我们证明了我们的MiniGPT-4可以推进 与GPT-4演示中展示的能力相似。</p>
<p>​    （2）仅仅训练一个投影层就可以有效地进行训练 y将视觉特征与大型语言模型对齐。</p>
<p>​    （3）公开数据质量低，不足以对齐，得加入一些超高质量数据。</p>
<p><strong>Framework</strong></p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230420134831112.png" >
        </sapn>
      </p>
<p>本文说图文对预训练（这里的预训练只训练mlp），minigpt4能有能力但是语言不完整，常常生成碎片化的重复的东西。我们还注意到，在GPT-3中也面临着类似的问题。尽管在广泛的语言数据集上进行了预训练，GPT-3仍然不能直接生成与用户的意图一致的语言输出，需要微调解决。</p>
<p>第一阶段：</p>
<p>图文对预训练，SBU和LAION</p>
<p>第二阶段：</p>
<p>用高质量数据集来微调，高质量图文对主要体现为文本的高质量</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230420135801654.png" >
        </sapn>
      </p>
<p>它采用prompt一次判断句子长度是否超出80tokens，如果超出则保存，否则加一个下面的prompt，把两次的句子连起来，这样得到5000个图文对。</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230420141256875.png" >
        </sapn>
      </p>
<p>然后又人工筛选出里面质量最高的3500条。</p>
<p>然后根据格式：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230420141441097.png" >
        </sapn>
      </p>
<p>进行二阶段训练。</p>
<p><strong>Limitations</strong></p>
<p>感知能力不足。MiniGPT-4的视觉感知仍然有限。它可能难以从图像中识别详细的文本信息，并区分空间定位。可能源于以下几个因素：</p>
<p>1)缺乏足够的对齐图像-文本数据，其中包含足够的信息，如空间定位和光学字符注释。这个问题可以通过对更一致和丰富的数据的训练来缓解</p>
<p>2)还有就是说整体匹配的视觉端就像Q-fomer和clip的vit这种会丢失很多视觉信息</p>
<p>3)只训练一个投影层可能无法提供足够的能力来学习广泛的视觉-文本对齐</p>
<h3 id="2-7-LIMA"><a href="#2-7-LIMA" class="headerlink" title="2.7.LIMA"></a>2.7.LIMA</h3><blockquote>
<p>大型语言模型的训练分为两个阶段： (1)从原始文本进行无监督预训练，学习通用表示，(2)大规模指令调优和强化学习，以更好地对齐任务和用户偏好。我们通过训练LIMA来衡量这两个阶段的相对重要性，LIMA是一个用标准supervised进行微调的65B参数的基于LLaMa的语言模型：只在1000个精心策划的提示和回复上进行了微调。</p>
</blockquote>
<p><strong>Instruction</strong></p>
<p>简单粗暴介绍了一个强的预训练语言模型经过高质量prompt对微调可以到达很强的性能，也就是说预训练阶段train进参数的知识，通过指令微调说出来。</p>
<p>消融实验显示，在不扩大数据多样性的情况下扩大数据量时，收益大大下降，同时在优化数据质量时也获得了主要收益。</p>
<p><strong>Experiment</strong></p>
<p>主要就是个人工标注和gpt4标注，说明了1000个高标准sft数据微调远比大规模sft来的实际，但是感觉效果其实也挺一般的。</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230522152232048.png" >
        </sapn>
      </p>
<p>下面这两个比较经典，证实了我在minigpt4的猜想（其实本质上sft就是在微调分布，这一个步骤并不需要太多数据，甚至可能一条数据反复拟合都能吧分布拉扯到一个合理的区间，特别适用于多语言模型，这就证明了gpt4报告中10%的中文但是他本质上中文并不差，因为语言之间本质上就存在翻译的逻辑，所以英文对应的中文永远是对应的，这种对应可以想象成分布的gap，只用利用高质量数据让两种语言分布产生交集即可），即使数据量增大，但是微调后的语言模型的输出质量并没有怎么提高（结果由gpt4评测）</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230522153329512.png" >
        </sapn>
      </p>
<p>这个对话也是：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230522153756587.png" >
        </sapn>
      </p>
<h3 id="2-8-VisionLLM"><a href="#2-8-VisionLLM" class="headerlink" title="2.8.VisionLLM"></a>2.8.VisionLLM</h3><blockquote>
<p>本文提出了一个统一的框架用于将多模态GPT类模型来做各种下游视觉任务</p>
</blockquote>
<p><strong>Introduction</strong></p>
<p>LLM的成功并不能很好得拓展到视觉，因为两个模态存在较大的差异。说现有的各种视觉任务指令多种多样，直接应用于多模态GPT难度过大，得不到理想化的输出，因此，迫切需要一个统一的框架，能够无缝地集成llm的优势与以视觉为中心的任务的特定需求。VisionLLM它将以视觉为中心的任务的定义与llm的方法相结合。它包括三个核心组成部分： (1) a unifified language instruction designed for vision and vision-language tasks；(2) a language-guided image tokenizer；(3) an LLM-based open-ended task decoder that orchestrates various tasks using language instructions。</p>
<p>以下列出了几种之前模型的范式：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230523115418905.png" >
        </sapn>
      </p>
<p><strong>Framework</strong></p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230523120008007.png" >
        </sapn>
      </p>
<p>看起来是简单有效，但是我其实挺怀疑他是否真能生成对应的结果不夹杂其他多余语句。</p>
<h3 id="2-9-Just-Ask-for-Calibration"><a href="#2-9-Just-Ask-for-Calibration" class="headerlink" title="2.9.Just Ask for Calibration"></a>2.9.Just Ask for Calibration</h3><blockquote>
<p>本研究对从RLHF-LMs中提取置信度分数的计算方法进行了广泛评估。通过合适的提示策略和温度缩放方法，作者发现可以将RLHF-LMs的期望校准误差降低50%以上，从而获得相当好的校准性能。</p>
</blockquote>
<p><strong>Introduction</strong></p>
<p>一个可信赖的预测系统不仅应该产生预测结果，而且应该提供良好校准的概率，即模型对预测结果的置信度应该准确反映预测结果正确的概率。对于语言模型来说，校准性不好会导致模型产生错误的事实或推理，即产生幻觉。然而，目前大型语言模型的能力已经得到了广泛关注，但它们的校准性相对较少受到关注。RLHF-LMs可能会以让对话更贴合用户指令为代价，而牺牲校准性。这是因为强化学习的目标是鼓励模型将尽可能多的概率质量分配给最可能的答案，而不是匹配可能答案的相对频率。本篇文章评估了从RLHF-LMs中提取置信度的一系列方法。我们发现，令人惊讶的是，流行的RLHF-LMs能够直接表达置信度分数，而这些置信度分数比模型的真实条件概率更好地校准（通过采样估计）。我们还发现，受到人类心理学研究的启发，即在回答前考虑备选答案可以减轻过度自信的影响，我们发现在给出置信度评分之前，提示模型提供几个答案选择可以显着改善口头概率的校准性。</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230612110833490.png" >
        </sapn>
      </p>
<p>这幅图我暂时白话理解为：如果在语言模型预测label这一类确定性的东西之前给到一定的cot或者候选答案，然后综合给出一个label，他会具有更高的校准率。</p>
<p><strong>Experiment</strong></p>
<p>作者从TriviaQA和SciQ的验证集中随机抽取了1000个问题，从TruthfulQA中选择了817个问题进行实验。</p>
<p>GPT3.5-turbo：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230612111729761.png" >
        </sapn>
      </p>
<p>GPT4：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230612111859613.png" >
        </sapn>
      </p>
<h3 id="2-10-ALIGNSCORE"><a href="#2-10-ALIGNSCORE" class="headerlink" title="2.10.ALIGNSCORE"></a>2.10.ALIGNSCORE</h3><blockquote>
<p>很多文本生成应用需要生成的文本与输入信息在事实上保持一致。本文提出了一种新的综合指标ALIGNSCORE，适用于各种上述的事实不一致性情况。ALIGNSCORE基于两个任意文本之间的信息对齐的通用函数。关键是，我们通过整合大量不同的数据源开发了对齐函数的统一训练框架，从七个成熟的任务（NLI、QA、释义、事实验证、信息检索、语义相似度和摘要生成）中获得了470万个训练样本。我们在包含22个评估数据集的大规模基准上进行了大量实验，其中19个数据集在对齐训练中从未见过。ALIGNSCORE在各种先前的指标上取得了显著的改进。</p>
</blockquote>
<p><strong>Introduction</strong></p>
<p>现在很多语言模型会生成很多事实性错误，因此，开发自动评估一个主张（例如生成的文本）与上下文（例如模型输入）的事实一致性的度量标准至关重要。在实验中，我们通过微调轻量级的RoBERTa模型（125M和355M）来构建ALIGNSCORE进行对齐。我们在最新的大规模评估基准上评估了ALIGNSCORE，包括SummaC（Laban等，2022）、TRUE（Honovich等，2022b）和其他测试集，总共包含22个具有挑战性的评估数据集。我们的方法在不同的质量指标上明显优于之前最先进的度量标准。值得注意的是，我们的度量标准（355M）与基于数量级更大的语言模型（如ChatGPT和GPT-4）的最新度量标准持平，有时甚至更好。特别是，ALIGNSCORE在19个零样本数据集上展现出很强的泛化能力，这些数据集在对齐函数训练过程中从未见过。我们还进行了广泛的消融研究，以展示上下文拆分策略和其他建模选择的有效性。</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306192228585.png" >
        </sapn>
      </p>
<p>给定两个文本片段a和b，我们认为当b中的所有信息都存在于a中并且不与a矛盾时，b与a对齐。从概念上讲，我们将信息对齐建模为一个将文本对(a, b)映射到标签y的函数，该标签描述了对齐的程度：</p>
<blockquote>
<p>f(a, b) —-&gt; score</p>
</blockquote>
<p>整体思路很像监督微调。</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306192231922.png" >
        </sapn>
      </p>
<p>其中:</p>
<p>ybin用于（重述、问答、信息检索和摘要）</p>
<p>y3way用于3类分类标签（NLI和事实验证）</p>
<p>yreg用于对于具有连续标签的任务（语义文本相似性）打分</p>
<p>总体构建了一个对齐模型(RoBERTa和三个线性层用于输出上面三个东西)，loss为：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306192232361.png" >
        </sapn>
      </p>
<h3 id="2-11-Can-Language-Models-Teach-Weaker-Agents"><a href="#2-11-Can-Language-Models-Teach-Weaker-Agents" class="headerlink" title="2.11.Can Language Models Teach Weaker Agents?"></a>2.11.Can Language Models Teach Weaker Agents?</h3><blockquote>
<p>研究了教师和学生双代理架构，教师何时以及如何通过自然语言解释干预来提高学生的表现。</p>
<p>学生问题分为四个方面：</p>
<p>（1）教师在测试时的干预是否改善了学生的预测</p>
<p>（2）何时值得解释一个数据点</p>
<p>（3）教师如何个性化解释以更好地教导学生</p>
<p>（4)教师的解释是否也能改善学生在未来未解释的数据上的表现</p>
<p>本文提出了两个点：<br>（1）一个干预函数，模拟干预的效用，在干预效用最高的时候对student进行干预，从而在较低效用下提高学生的表现</p>
<p>（2）证明教师给到学生解释能提升学生性能，并证明了教师给到学生错误的解释会降低学生的性能甚至会使他达到随机。</p>
</blockquote>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306192238154.png" >
        </sapn>
      </p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306192238704.png" >
        </sapn>
      </p>
<p>本研究旨在评估大型语言模型（LLMs）在解决推理任务中教导和改善较弱代理的能力，并提出了一个学生-教师框架。研究问题包括：</p>
<p>RQ1：教师LLM在测试时是否能够干预并改善学生LLM的预测？</p>
<p>RQ2：在固定的干预预算下，教师何时干预（即选择哪些数据点）以最大化学生表现？</p>
<p>RQ3：给定一组干预数据，教师模型能否使用心理理论来个性化解释以改善学生表现？</p>
<p>RQ4：在多轮交互中，教师解释是否推广并改善学生在未解释样本上的表现？</p>
<p>RQ5：教师LLMs是否会通过向学生提供误导性解释而降低学生表现？</p>
<p>研究发现，教师LLMs能够有效地干预学生的推理过程，并改善学生在任务上的表现。利用心理理论，提出了一种干预函数，使教师能够根据学生的推理过程选择最有帮助的干预样本。同时，研究还表明，个性化解释对学生模型的表现有积极影响，而随机解释效果较差。此外，教师解释能够推广到未解释的数据，并提高学生在这些数据上的表现。</p>
<p>然而，研究还发现，如果教师LLMs提供虚假解释，并且学生直接采纳这些解释，将会降低学生的表现。这对于其他代理在错误地信任这些解释的情况下给出解释的LLMs可能具有潜在的负面影响。</p>
<p>总之，这项研究全面地展示了LLMs教导和改善较弱LLMs的能力，不仅在解释的测试样本上有改进，而且在未解释的数据上也有提升。</p>
<h3 id="2-12-The-False-Promise-of-Imitating-Proprietary-LLMs"><a href="#2-12-The-False-Promise-of-Imitating-Proprietary-LLMs" class="headerlink" title="2.12.The False Promise of Imitating Proprietary LLMs"></a>2.12.The False Promise of Imitating Proprietary LLMs</h3><blockquote>
<p>本文研究了利用更弱的开源语言模型在更强大的专有模型（如ChatGPT）的输出上进行微调，以便廉价地改进较弱的语言模型。我们首先使用不同的基础模型大小（1.5B-13B）、数据源和模仿数据量（0.3M-150M个标记）对一系列模型进行微调，模仿ChatGPT。然后，我们使用评估员和经典的自然语言处理基准测试对这些模型进行评估。总体而言，我们得出结论：模型模仿是一个虚假的承诺。开源语言模型和闭源语言模型之间存在着实质性的能力差距，目前的方法只能通过大量的模仿数据或使用更强大的基础语言模型来弥合这一差距，而这种方法往往不太可行。因此，我们认为提高开源模型的最有效方法是应对开发更好的基础语言模型这一艰巨的挑战，而不是采用模仿专有系统的捷径。</p>
</blockquote>
<p><strong>Introduction</strong></p>
<p>在这项工作中，我们研究了解决这个问题的一个可能方法：模型模仿。模型模仿的前提是，一旦通过API提供专有的语言模型，就可以收集API的输出数据集，并将其用于对开源语言模型进行微调。理论上，这种模仿过程可以轻松地提取任何专有模型的功能，从而暗示开源语言模型将始终与商业模型竞争，例如，Alpaca。</p>
<p>本文研究了1.5B-13B各种的LLM，主要使用人工评估和GPT-4评估（与ChatGPT进行盲目成对比较）以及在经典NLP基准测试（MMLU、NQ、HumanEval）上的准确性进行评估。</p>
<p>发现：遵循指令上和chatgpt相当，但是在专有的评测上他们还是很弱的。我们的主要观点是，模型模仿并非是一种免费午餐：当今的开源语言模型与闭源模型之间存在着能力差距，这个差距不能通过廉价的模仿数据微调来弥合。事实上，我们发现通过增加基础语言模型的规模等方式来缩小这个能力差距比额外的模仿数据微调更有效。</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306211548241.png" >
        </sapn>
      </p>
<p>总体的实验结果如下：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306211623746.png" >
        </sapn>
      </p>
<p>总体来说就是<strong>能在见过的能力表现好，但是不全面，有些任务甚至会下降。同时主要模仿了GPT的说话方式，并没有太模仿内容（简而言之就是这些模仿的模型自信满满的讲出了一堆屁话）</strong>，同时在微调过程中，甚至要避免引入新的知识，否则模型可能会猜测或虚构答案，而不是按照预期执行任务。</p>
<h3 id="2-13-Visual-Programming-for-Text-to-Image-Generation-and-Evaluation"><a href="#2-13-Visual-Programming-for-Text-to-Image-Generation-and-Evaluation" class="headerlink" title="2.13.Visual Programming for Text-to-Image Generation and Evaluation"></a>2.13.Visual Programming for Text-to-Image Generation and Evaluation</h3><blockquote>
<p>本文提出了一个VPGEN，一个可解释的逐步T2I生成框架，将T2I生成分解为三个步骤：对象/计数生成、布局生成和图像生成。然后引入了VPEVAL，一个可解释和可解释的T2I生成评估框架，基于视觉编程。</p>
</blockquote>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202307051510469.png" >
        </sapn>
      </p>
<p>研究提出了两种新颖的可解释/可解释的视觉编程(VP)框架，将LLMs和视觉模块结合起来进行T2I生成和评估。作者使用了Vicuna，通过在多个数据集的文本-布局对上进行微调来处理前两个步骤(对象/计数生成和布局生成)，从而改进了T2I生成的布局控制。对于最后的图像生成步骤，我们使用现成的布局到图像生成模型，例如GLIGEN。我们的生成框架提供了比广泛使用的端到端T2I模型更可解释的空间控制能力。</p>
<p>其次本文提出了VPEVAL，一个新的可解释/可解释的T2I评估框架，基于评估程序调用不同的视觉模块来评估不同的T2I技能，并提供视觉+文本的评估结果解释。</p>
<p>本文的VPGEN的前两步骤生成对象计数和布局都用Vicuna 13B在Flickr30K，MS COCO 2014和PaintSkills上专门微调过。之后的整体流程就如下：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202307051507513.png" >
        </sapn>
      </p>
<p>评估流程：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202307051547619.png" >
        </sapn>
      </p>
<p>这个评价体系还是挺全面的，比TIFA靠谱点：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202307051550455.png" >
        </sapn>
      </p>

        <!-- 分类文章 -->
        
      </div>
      <div class="post-content-inner-space">
        
          <div class="space-toc-main animate__animated  animate__fadeInUp">
            <ol class="space-toc"><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#1-%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE%E6%80%BB%E7%BB%93"><span class="space-toc-text">1.相关文献总结</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#2-%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0"><span class="space-toc-text">2.文献阅读笔记</span></a></li></ol>
           </div>
        
      </div>
   </div>
    <!-- 评论 -->
    
  </div>
</article>
  </div>
</div>



<!-- 如果是home模式的话，不在首页就显示footer，如果不是home模式的话 所有都显示footer -->

  <div class="footer-outer animate__animated  animate__fadeInUp">
    <div class="footer-inner">
    <div class="footer-text">
    <p>Power by <a target="_blank" rel="noopener" href="http://hexo.io/">Hexo</a> Theme by <a target="_blank" rel="noopener" href="https://github.com/FuShaoLei/hexo-theme-white">White</a></p>

    </div>
    <div class="footer-contact">
    <ul class="footer-ul">
        
        <li class="footer-li">
            <a href="https://github.com/YiyangZhou" target="_blank">
                <i class="ri-github-line"></i>
            </a>
        </li>
        
        <li class="footer-li">
            <a href="mailto:zhouyiyangailab@gmail.com" target="_blank">
                <i class="ri-mail-line"></i>
            </a>
        </li>
        
        <li class="footer-li">
            <a href="https://scholar.google.com/citations?user=6KltFMAAAAAJ&amp;hl=zh-CN" target="_blank">
                <i class="ri-google-fill"></i>
            </a>
        </li>
        
    </ul>
    </div>
    </div>
</div>





<script src="/js/white.js"></script>



</body>
</html>
