


<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>  Megatron-LM |    Yiyang Ai Lab.</title>
  <meta name="description" content="Personal website. Recording some creative things">
  <!-- 标签页图标 -->
  
  <link rel="shortcut icon" href="https://github.com/YiyangZhou/imageBeds/blob/main/imgs/collage-line.png" type="image/x-icon">
  

  <!-- 图标库 -->
  <link href="https://cdn.jsdelivr.net/npm/remixicon@2.2.0/fonts/remixicon.css" rel="stylesheet">
  <!-- 动画库 -->
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fushaolei/cdn-white@1.0/css/animate.css"/>
  
  <!-- css文件 -->
  
<link rel="stylesheet" href="/css/white.css">

  <!-- 代码高亮 -->
  
<meta name="generator" content="Hexo 5.4.2"></head>


<body>

<div class="menu-outer">
    <div class="menu-inner">
      <div class="menu-site-name  animate__animated  animate__fadeInUp">
        <a href="/">
          Yiyang Ai Lab.
        </a>
        
      </div>
      <div class="menu-group">
        <ul class="menu-ul">
        
          <a href="/" class="nav-link">
            <li class="menu-li  animate__animated  animate__fadeInUp">
              HOME
            </li>
          </a>
        
          <a href="/archives" class="nav-link">
            <li class="menu-li  animate__animated  animate__fadeInUp">
              BLOG
            </li>
          </a>
        
        
        
          <li class="menu-li animate__animated  animate__fadeInUp" id="mobile-menu">
            <i class="ri-menu-line"></i>
          </li>
        
        </ul>

      </div>

    </div>
</div>
<div id="mobile-main" class="animate__animated  animate__fadeIn">
  <div class="mobile-menu-inner">
    <div class="mobile-menu-site-name animate__animated  animate__fadeInUp">
      <a href="/">
        Yiyang Ai Lab.
      </a>
    </div>
    <div class="mobile-menu-group" id="mobile-close">
      <i class="ri-close-line"></i>
    </div>

  </div>

  <div class="mobile-menu-div">
  
    <a href="/" class="mobile-nav-link">
      <div class="mobile-menu-child animate__animated  animate__fadeInUp">
        <span>HOME</span>
      </div>
    </a>
  
    <a href="/archives" class="mobile-nav-link">
      <div class="mobile-menu-child animate__animated  animate__fadeInUp">
        <span>BLOG</span>
      </div>
    </a>
  
  
  </div>


</div>

<div class="body-outer">
  <div class="body-inner">
    
<article class="post-inner">
  <div class="post-content-outer">
    <div class="post-intro">
      <div class="post-title animate__animated  animate__fadeInUp">Megatron-LM</div>
      <div class="meta-intro animate__animated  animate__fadeInUp">Jun 06 2023</div>
      
    </div>
    <div class="post-content-inner">
      <div class="post-content-inner-space">

      </div>
      <div class="post-content-main animate__animated  animate__fadeInUp">
        <!-- top型目录 -->
        
        <h1 id="Megatron-LM"><a href="#Megatron-LM" class="headerlink" title="Megatron-LM"></a>Megatron-LM</h1><p>整个知识摘抄于，摘抄一遍记忆深刻，这个博客写的是真好：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/613196255">https://zhuanlan.zhihu.com/p/613196255</a></p>
<h2 id="1-什么是Megatron-LM？"><a href="#1-什么是Megatron-LM？" class="headerlink" title="1.什么是Megatron-LM？"></a>1.什么是Megatron-LM？</h2><p>Megatron-LM是一个基于pytorch的框架，专门用于训练基于transformer架构的大语言模型。</p>
<p>他通过以下几种方式实现了高效的大语言模型训练：</p>
<p>1.模型并行：将模型参数分散在多个GPU上，减少单个GPU的内存占用。</p>
<p>2.数据并行：将数据批次分散在多个GPU上，增加训练吞吐量。</p>
<p>3.混合精度：使用16位浮点数代替32位浮点数，减少内存和带宽需求，提高计算速度。</p>
<p>4.梯度累积：在多个数据批次上累计梯度，然后再更新参数，降低通讯开销。</p>
<p>5.Megatron-LM还可以与其他框架如DeepSpeed结合，实现更高级的并行：ZeRO分片和管道并行。</p>
<h2 id="2-前缀知识"><a href="#2-前缀知识" class="headerlink" title="2.前缀知识"></a>2.前缀知识</h2><h3 id="2-1-流水线并行（Pipeline-Parallelism）"><a href="#2-1-流水线并行（Pipeline-Parallelism）" class="headerlink" title="2.1.流水线并行（Pipeline Parallelism）"></a>2.1.流水线并行（Pipeline Parallelism）</h3><p>本篇文章将探索流水线并行，经典的流水线并行范式有Google推出的Gpipe，和微软推出的PipeDream。两者的推出时间都在2019年左右，大体设计框架一致。主要差别为：在梯度更新上，Gpipe是同步的，PipeDream是异步的。异步方法更进一步降低了GPU的空转时间比。虽然PipeDream设计更精妙些，但是Gpipe因为其“够用”和浅显易懂，更受大众欢迎（torch的pp接口就基于Gpipe）。因此本文以Gpipe作为流水线并行的范例进行介绍。</p>
<p>本文主要内容包括：</p>
<p>1、优化目标<br>2、模型并行<br>3、流水线并行</p>
<ul>
<li>切分micro-batch</li>
<li>Re-materialization (active checkpoint)</li>
</ul>
<p><strong>1、优化目标</strong></p>
<p>难点：</p>
<ul>
<li>训练更大的模型时，每块GPU里不仅要存模型参数，还要存中间结果（用来做Backward）。而更大的模型意味着需要更多的训练数据，进一步提高了中间结果的大小。加重了每块GPU的内存压力。</li>
<li>网络通讯开销。数据在卡之间进行传输，是需要通讯时间的。不做设计的话，这个通讯时间可能会抹平多卡本身带来的训练速度提升。</li>
</ul>
<p><strong>2.模型并行</strong></p>
<p>最简单最直接的方法是将模型分层，因为网络有很多层，将不同的层放到不同的GPU上。</p>
<p>但是这样的话（每行表示一个gpu，每一列代表一个时间步）：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230614115218668.png" >
        </sapn>
      </p>
<p>这张图的含义是：我在GPU0上做完一次forward，然后将GPU0上最后一层的输入传给GPU1，继续做forward，直到四块GPU都做完forward后，我再依次做backward。等把四块GPU上的backward全部做完后，最后一个时刻我统一更新每一层的梯度。</p>
<p>这种方法会有缺陷：</p>
<p>==GPU利用率低==</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230614115610343.png" >
        </sapn>
      </p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230614115631260.png" >
        </sapn>
      </p>
<p>==中间结果占用大量内存==</p>
<p>因为每层需要存储每一层的结果，这样会导致每个gpu都会占用额外的一部分内存来存储这一结果，如果每一层的宽度太大（也就是模型变大）会导致这一个额外的存储太大平滑掉GPU增多带来的收益。</p>
<p><strong>3.流水线并行</strong></p>
<p>Gpipe提出了流水线并行，来解决上述两个问题</p>
<p>3.1.切分micro-batch</p>
<p>流水线并行的核心思想是：<strong>在模型并行的基础上，进一步引入数据并行的办法，即把原先的数据再划分成若干个batch，送入GPU进行训练</strong>。未划分前的数据，叫<strong>mini-batch</strong>。在mini-batch上再划分的数据，叫<strong>micro-batch</strong>。</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230614131618033.png" >
        </sapn>
      </p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230614131709553.png" >
        </sapn>
      </p>
<p>3.2.re-materialization（active checkpoint）</p>
<p>解决了GPU的空置问题，提升了GPU计算的整体效率。接下来，就要解决GPU的内存问题了。<br>前文说过，随着模型的增加，每块GPU中存储的中间结果也会越大。对此，Gpipe采用了一种非常简单粗暴但有效的办法：<strong>用时间换空间，在论文里，这种方法被命名为re-materalization，后人也称其为active checkpoint</strong>。<br>具体来说，就是<strong>几乎不存中间结果，等到backward的时候，再重新算一遍forward</strong>，图例如下：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230614132106246.png" >
        </sapn>
      </p>
<p>每块GPU上，我们只保存来自上一块的最后一层输入z，其余的中间结果我们算完就废。等到backward的时候再由保存下来的z重新进行forward来算出。<br>现在我们来计算每块GPU峰值时刻的内存：<br><strong>每块GPU峰值时刻存储大小 = 每块GPU上的输入数据大小 + 每块GPU在forward过程中的中间结果大小</strong></p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230614132131221.png" >
        </sapn>
      </p>
<p>最后，再提一点，在micro-batch的划分下，我们在计算<strong>Batch Normalization</strong>时会有影响。Gpipe的方法是，在训练时计算和运用的是micro-batch里的均值和方差，但同时持续追踪全部mini-batch的移动平均和方差，以便在测试阶段进行使用。Layer Normalization则不受影响。</p>
<h3 id="2-2-数据并行（DP-DDP）"><a href="#2-2-数据并行（DP-DDP）" class="headerlink" title="2.2.数据并行（DP,DDP）"></a>2.2.数据并行（DP,DDP）</h3><p>实际应用中，流水线并行并不特别流行，主要原因是模型能否均匀切割，影响了整体计算效率，这就需要算法工程师做手调。<strong>因此，今天我们来介绍一种应用最广泛，最易于理解的并行范式：数据并行。</strong></p>
<p>数据并行的核心思想是：<strong>在各个GPU上都拷贝一份完整模型，各自吃一份数据，算一份梯度，最后对梯度进行累加来更新整体模型</strong>。理念不复杂，但到了大模型场景，<strong>巨大的存储和GPU间的通讯量，</strong>就是系统设计要考虑的重点了。在本文中，我们将递进介绍三种主流数据并行的实现方式：</p>
<ul>
<li><strong>DP（Data Parallelism）</strong>：最早的数据并行模式，一般采用参数服务器(Parameters Server)这一编程框架。实际中多用于单机多卡</li>
<li><strong>DDP（Distributed Data Parallelism）</strong>：分布式数据并行，采用Ring AllReduce的通讯方式，实际中多用于多机场景</li>
<li><strong>ZeRO：</strong>零冗余优化器。由微软推出并应用于其DeepSpeed框架中。严格来讲ZeRO采用数据并行+张量并行的方式，旨在降低存储。</li>
</ul>
<p><strong>2.2.1.DP</strong></p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230614133648185.png" >
        </sapn>
      </p>
<p>一个经典数据并行的过程如下：</p>
<ul>
<li>若干块<strong>计算GPU</strong>，如图中GPU0~GPU2；1块<strong>梯度收集GPU</strong>，如图中AllReduce操作所在GPU。</li>
<li>在每块计算GPU上都拷贝一份完整的模型参数。</li>
<li>把一份数据X（例如一个batch）均匀分给不同的计算GPU。</li>
<li>每块计算GPU做一轮FWD和BWD后，算得一份梯度G。</li>
<li>每块计算GPU将自己的梯度<strong>push</strong>给梯度收集GPU，做聚合操作。这里的聚合操作一般指<strong>梯度累加</strong>。当然也支持用户自定义。</li>
<li>梯度收集GPU聚合完毕后，计算GPU从它那<strong>pull</strong>下完整的梯度结果，用于更新模型参数W。更新完毕后，计算GPU上的模型参数依然保持一致。</li>
<li><strong>聚合再下发梯度的操作，称为AllReduce</strong>。</li>
</ul>
<p>前文说过，实现DP的一种经典编程框架叫“参数服务器”，在这个框架里，<strong>计算GPU称为Worker</strong>，<strong>梯度聚合GPU称为Server。</strong>在实际应用中，为了尽量减少通讯量，一般可选择一个Worker同时作为Server。比如可把梯度全发到GPU0上做聚合。需要再额外说明几点：</p>
<ul>
<li>1个Worker或者Server下可以不止1块GPU。</li>
<li>Server可以只做梯度聚合，也可以梯度聚合+全量参数更新一起做</li>
</ul>
<p>==存在的问题==</p>
<p>DP的框架理解起来不难，但实战中确有两个主要问题：</p>
<ul>
<li><p><strong>存储开销大</strong>。每块GPU上都存了一份完整的模型，造成冗余。关于这一点的优化，我们将在后文ZeRO部分做讲解。</p>
</li>
<li><p><strong>通讯开销大</strong>。Server需要和每一个Worker进行梯度传输。当Server和Worker不在一台机器上时，Server的带宽将会成为整个系统的计算效率瓶颈。</p>
</li>
<li><p><strong>worker摸鱼</strong>。如果集中push完然后等待pull操作，那么所有的worker都在摸鱼。</p>
</li>
</ul>
<p>为了解决第三个问题提出了<strong>梯度异步更新</strong></p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230614134335872.png" >
        </sapn>
      </p>
<p>上图刻画了在<strong>梯度异步更新</strong>的场景下，某个Worker的计算顺序为：</p>
<ul>
<li>在第10轮计算中，该Worker正常计算梯度，并向Server发送push&amp;pull梯度请求。</li>
<li>但是，该Worker并不会实际等到把聚合梯度拿回来，更新完参数W后再做计算。而是直接拿旧的W，吃新的数据，继续第11轮的计算。<strong>这样就保证在通讯的时间里，Worker也在马不停蹄做计算，提升计算通讯比。</strong></li>
<li>当然，异步也不能太过份。只计算梯度，不更新权重，那模型就无法收敛。图中刻画的是<strong>延迟为1</strong>的异步更新，也就是在开始第12轮对的计算时，必须保证W已经用第10、11轮的梯度做完2次更新了。</li>
</ul>
<p>参数服务器的框架下，延迟的步数也可以由用户自己决定，下图分别刻划了几种延迟情况：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230614134627712.png" >
        </sapn>
      </p>
<p>总结一下，<strong>异步很香，但对一个Worker来说，只是等于W不变，batch的数量增加了而已，在SGD下，会减慢模型的整体收敛速度</strong>。异步的整体思想是，比起让Worker闲着，倒不如让它多吃点数据，虽然反馈延迟了，但只要它在干活在学习就行。<br>batch就像活，异步就像画出去的饼，且往往不指定延迟步数，每个Worker干越来越多的活，但模型却没收敛。</p>
<p><strong>2.2.2.DDP</strong></p>
<p>受通讯负载不均的影响，<strong>DP一般用于单机多卡场景</strong>。因此，DDP作为一种更通用的解决方案出现了，既能多机，也能单机。<strong>DDP首先要解决的就是通讯问题：将Server上的通讯压力均衡转到各个Worker上。实现这一点后，可以进一步去Server，留Worker。</strong><br>前文我们说过，聚合梯度 + 下发梯度这一轮操作，称为AllReduce。<strong>接下来我们介绍目前最通用的AllReduce方法：Ring-AllReduce</strong>。它由百度最先提出，非常有效地解决了数据并行中通讯负载不均的问题，使得DDP得以实现。</p>
<p>AllReduc类似于：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306201130885.png" >
        </sapn>
      </p>
<p>但是对于Ring-AllReduce则分为两大步骤实现该目标：<strong>Reduce-Scatter</strong>和<strong>All-Gather。</strong></p>
<p><strong>Reduce-Scatter</strong><br>定义网络拓扑关系，使得每个GPU只和其相邻的两块GPU通讯。每次发送对应位置的数据进行<strong>累加</strong>。每一次累加更新都形成一个拓扑环，因此被称为Ring。看到这觉得困惑不要紧，步骤类似于这样，这里又想夸作者了，写得真好：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306201132889.png" >
        </sapn>
      </p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306201133370.png" >
        </sapn>
      </p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306201133406.png" >
        </sapn>
      </p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306201134140.png" >
        </sapn>
      </p>
<p><strong>3次</strong>更新之后，每块GPU上都有一块数据拥有了对应位置完整的聚合（图中红色）。此时，Reduce-Scatter阶段结束。进入All-Gather阶段。目标是把红色块的数据广播到其余GPU对应的位置上。</p>
<p><strong>All-Gather</strong><br>如名字里Gather所述的一样，这操作里依然按照“相邻GPU对应位置进行通讯”的原则，但对应位置数据不再做相加，而是直接替换。All-Gather以红色块作为起点。</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306201135723.png" >
        </sapn>
      </p>
<p>以此类推，同样经过<strong>3轮迭代后</strong>，使得每块GPU上都汇总到了完整的数据，变成如下形式：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306201135189.png" >
        </sapn>
      </p>
<p><strong>Ring-AllReduce的通讯量分析</strong></p>
<p>假设model的参数量是&amp;，GPU的个数为N，梯度的大小总量为w，那么每个GPU的梯度大小为W/N（只算其通讯量）：</p>
<ul>
<li>Reduce-Scatter阶段的通讯量为：（N - 1）（&amp;/N）</li>
<li>All-Gather阶段的通讯量为：（N - 1）(&amp;/N)</li>
</ul>
<p>其中的N-1来源于上述图的推导。所以单卡的通讯量为2（N - 1）(&amp;/N) –&gt; 2&amp;，总的通讯量为2N&amp;。</p>
<p>这和之前的All-Reduce的通讯量一样的，但是其中的Server只用来当搬运工，当越来越多的GPU分布到较远的机器上时，DP的通讯时间是很大的。</p>
<p><strong>总结</strong></p>
<p>1、在DP中，每个GPU上都拷贝一份完整的模型，每个GPU上处理batch的一部分数据，所有GPU算出来的梯度进行累加后，再传回各GPU用于更新参数<br>2、DP多采用参数服务器这一编程框架，一般由若个计算Worker和1个梯度聚合Server组成。Server与每个Worker通讯，Worker间并不通讯。因此Server承担了系统所有的通讯压力。基于此DP常用于单机多卡场景。<br>3、异步梯度更新是提升计算通讯比的一种方法，延迟更新的步数大小决定了模型的收敛速度。<br>4、Ring-AllReduce通过定义网络环拓扑的方式，将通讯压力均衡地分到每个GPU上，使得跨机器的数据并行（DDP）得以高效实现。<br>5、DP和DDP的总通讯量相同，但因负载不均的原因，DP需要耗费更多的时间搬运数据。</p>
<h3 id="2-3-ZeRO（零冗余优化）"><a href="#2-3-ZeRO（零冗余优化）" class="headerlink" title="2.3.ZeRO（零冗余优化）"></a>2.3.ZeRO（零冗余优化）</h3><p>之前两种方法DP和DDP，DP存在负载不均的问题，但是他俩还存在一个共同问题就是显存开销问题，数据并行中，每个GPU上都复制了一份完整模型，当模型变大时，很容易打爆GPU的显存。</p>
<p>下面介绍一下微软开发的<strong>ZeRO（零冗余优化）</strong>，它是DeepSpeed这一分布式训练框架的核心，被用来解决大模型训练中的显存开销问题。他的主旨思想就是<strong>通讯换显存</strong>。</p>
<p><strong>2.3.1.训练过程中显卡需要存储的东西</strong></p>
<p>存储主要分为两大块：Model States和Residual States<br><strong>Model States</strong>指和模型本身息息相关的，必须存储的内容，具体包括：</p>
<ul>
<li><strong>optimizer states</strong>：Adam优化算法中的momentum和variance</li>
<li><strong>gradients</strong>：模型梯度</li>
<li><strong>parameters</strong>：模型参数W</li>
</ul>
<p><strong>Residual States</strong>指并非模型必须的，但在训练过程中会额外产生的内容，具体包括：</p>
<ul>
<li><strong>activation</strong>：激活值。在流水线并行中我们曾详细介绍过。在backward过程中使用链式法则计算梯度时会用到。有了它算梯度会更快，但它不是必须存储的，因为可以通过重新做Forward来算它。</li>
<li><strong>temporary buffers:</strong> 临时存储。例如把梯度发送到某块GPU上做加总聚合时产生的存储。</li>
<li><strong>unusable fragment memory</strong>：碎片化的存储空间。虽然总存储空间是够的，但是如果取不到连续的存储空间，相关的请求也会被fail掉。对这类空间浪费可以通过内存整理来解决。</li>
</ul>
<p><strong>2.3.2.精度混合训练</strong></p>
<p>对于模型，我们肯定希望其参数越精准越好，也即我们用<strong>fp32（单精度浮点数，存储占4byte）</strong>来表示参数W。但是在forward和backward的过程中，fp32的计算开销也是庞大的。那么能否在计算的过程中，引入<strong>fp16或bf16（半精度浮点数，存储占2byte）</strong>，来减轻计算压力呢？于是，混合精度训练就产生了，它的步骤如下图：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306201201271.png" >
        </sapn>
      </p>
<ul>
<li>存储一份fp32的parameter，momentum和variance（统称model states）</li>
<li>在forward开始之前，额外开辟一块存储空间，将fp32 parameter减半到fp16 parameter。</li>
<li>正常做forward和backward，在此之间产生的activation和gradients，都用fp16进行存储。</li>
<li>用fp16 gradients去更新fp32下的model states。</li>
<li>当模型收敛后，fp32的parameter就是最终的参数输出。</li>
</ul>
<p><strong>2.3.3.显存大小</strong></p>
<p>现在，我们可以来计算模型在训练时需要的存储大小了，假设模型的参数W大小是 Φ ，<strong>以byte为单位</strong>，存储如下：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306201206441.png" >
        </sapn>
      </p>
<p>因为采用了Adam优化，所以才会出现momentum和variance，当然你也可以选择别的优化办法。因此这里为了更通用些，记模型必存的数据大小为 KΦ 。因此最终内存开销为： 2Φ+2Φ+KΦ</p>
<p>另外，这里暂时不将activation纳入统计范围，原因是：</p>
<ul>
<li>activation不仅与模型参数相关，还与batch size相关。</li>
<li>activation的存储不是必须的。存储activation只是为了在用链式法则做backward的过程中，计算梯度更快一些。但你永远可以通过只保留最初的输入X，重新做forward来得到每一层的activation（虽然实际中并不会这么极端）。</li>
<li>因为activation的这种灵活性，纳入它后不方便衡量系统性能随模型增大的真实变动情况。因此在这里不考虑它，在后面会单开一块说明对activation的优化。</li>
</ul>
<p><strong>2.3.4.ZeRO-DP</strong></p>
<p>知道了什么东西会占存储，以及它们占了多大的存储之后，我们就可以来谈如何优化存储了。注意到，在整个训练中，有很多states并不会每时每刻都用到，举例来说；</p>
<ul>
<li>Adam优化下的optimizer states只在最终做update时才用到</li>
<li>数据并行中，gradients只在最后做AllReduce和updates时才用到</li>
<li>参数W只在做forward和backward的那一刻才用到</li>
<li>诸如此类</li>
</ul>
<p>所以看到了并不是所有东西在任何地方都会马上用到的，ZeRO的思想：<strong>如果数据算完即废，等需要的时候，我再想办法从个什么地方拿回来，那不就省了一笔存储空间吗？</strong></p>
<p><strong>P<del>os</del>:优化状态分割</strong></p>
<p>首先，从 optimizer state开始优化。将optimizer state分成若干份，每块GPU上各自维护一份。这样就减少了相当一部分的显存开销。如下图：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306251513237.webp" >
        </sapn>
      </p>
<p>复习一下，此时W=fp16，G=fp16，O=fp32。此时，整体数据并行的流程如下：</p>
<p>（1）每块GPU上存一份完整的参数W。将一个batch的数据分成3份，每块GPU各吃一份，做完一轮foward和backward后，各得一份梯度。</p>
<p>（2）对梯度做一次<strong>AllReduce</strong>，<strong>得到完整的梯度G</strong>，产生单卡通讯量 2Φ 。为了表达简明，这里通讯量我们就不再换算成byte了，而直接根据参数量来计算。</p>
<p>（3）得到完整梯度G，就可以对W做更新。我们知道W的更新由optimizer states和梯度共同决定。<strong>由于每块GPU上只保管部分optimizer states，因此只能将相应的W（蓝色部分）进行更新</strong>。（2）和（3）可以用下图表示：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306251514839.webp" >
        </sapn>
      </p>
<p>（4）此时，每块GPU上都有部分W没有完成更新（图中白色部分）。所以我们需要对W做一次<strong>All-Gather</strong>，从别的GPU上把更新好的部分W取回来。产生单卡通讯量 Φ 。</p>
<p>做完P<del>os</del>后，设GPU个数为N<del>d</del>，显存和通讯量的情况如下：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306251518938.webp" >
        </sapn>
      </p>
<p>这么一看通过变成1.5倍的单卡通讯量可以使得存储的显存降低四倍。</p>
<p><strong>P<del>os</del>+P<del>g</del>:优化状态与梯度分割</strong></p>
<p>因为之前的P<del>os</del>是通过AllReduce得到了整个梯度G，现在通过把梯度也拆开降低显存消耗：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306251522300.webp" >
        </sapn>
      </p>
<p>此时，数据并行的整体流程如下：<br>（1）每块GPU上存一份完整的参数W。将一个batch的数据分成3份，每块GPU各吃一份，做完一轮foward和backward后，<strong>算得一份完整的梯度（下图中绿色+白色）</strong>。<br>（2）对梯度做一次<strong>Reduce-Scatter</strong>，保证每个GPU上所维持的那块梯度是聚合梯度。例如对GPU1，它负责维护G1，因此其他的GPU只需要把G1对应位置的梯度发给GPU1做加总就可。汇总完毕后，白色块对GPU无用，可以从显存中移除。单卡通讯量 Φ 。（1）和（2）见下图：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306251524089.webp" >
        </sapn>
      </p>
<p>（3）每块GPU用自己对应的O和G去更新相应的W。更新完毕后，<strong>每块GPU维持了一块更新完毕的W</strong>。同理，对W做一次<strong>All-Gather</strong>，将别的GPU算好的W同步到自己这来。单卡通讯量 Φ <strong>。</strong></p>
<p>再次比对下显存和通讯量：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306251525663.webp" >
        </sapn>
      </p>
<p><strong>P<del>os</del>+P<del>g</del>+P<del>p</del>:优化状态、梯度与参数分割</strong></p>
<p>现在要将参数一起分割了</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306251529011.webp" >
        </sapn>
      </p>
<p>数据并行的流程如下：<br>（1）每块GPU上只保存部分参数W。将一个batch的数据分成3份，每块GPU各吃一份。<br>（2）做forward时，对W做一次<strong>All-Gather</strong>，取回分布在别的GPU上的W，得到一份完整的W，单卡通讯量 Φ <strong>。forward做完，立刻把不是自己维护的W抛弃。</strong><br>（3）做backward时，对W做一次<strong>All-Gather</strong>，取回完整的W，单卡通讯量 Φ <strong>。backward做完，立刻把不是自己维护的W抛弃。</strong><br>（4）做完backward，算得一份完整的梯度G，对G做一次<strong>Reduce-Scatter</strong>，从别的GPU上聚合自己维护的那部分梯度，单卡通讯量 Φ <strong>。聚合操作结束后，立刻把不是自己维护的G抛弃</strong>。<br>（5）用自己维护的O和G，更新W。由于只维护部分W，因此无需再对W做任何AllReduce操作。</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306251531257.webp" >
        </sapn>
      </p>
<p>到这一步，<strong>我们用1.5倍的通讯开销，换回近120倍的显存</strong>。只要梯度计算和异步更新做的好，通讯时间大部分可以被计算时间隐藏，因此这样的额外通讯开销，也是划算的。</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306251534184.webp" >
        </sapn>
      </p>
<p><strong>ZeRO VS 模型并行</strong></p>
<p>知道模型并行的朋友，可能会想，既然ZeRO都把参数W给切了，那它应该是个模型并行呀？为什么要归到数据并行里呢？<br>其实<strong>ZeRO是模型并行的形式，数据并行的实质</strong>。<br>模型并行，是指在forward和backward的过程中，我只需要用自己维护的那块W来计算就行。即<strong>同样的输入X，每块GPU上各算模型的一部分，最后通过某些方式聚合结果</strong>。<br>但对ZeRO来说，它做forward和backward的时候，是需要把各GPU上维护的W聚合起来的，即本质上还是用完整的W进行计算。<strong>它是不同的输入X，完整的参数W，最终再做聚合</strong>。</p>
<p><strong>2.3.5.ZeRO-R</strong></p>
<p>现在做对residual states的优化</p>
<p><strong>P<del>a</del>:Partitioned Activation Checkpointing</strong></p>
<p>activation对显存的占用一般会远高于模型本身，通讯量也是巨大的，所以这块要灵活、有效地实验设计。</p>
<p><strong>C<del>B</del>:Constant Size Buffer</strong></p>
<p>固定大小的内存buffer，它的目的在于：</p>
<ul>
<li>提升带宽利用率。当GPU数量上升，GPU间的通讯次数也上升，每次的通讯量可能下降（但总通讯量不会变）。数据切片小了，就不能很好利用带宽了。所以这个buffer起到了积攒数据的作用：等数据积攒到一定大小，再进行通讯。</li>
<li>使得存储大小可控。在每次通讯前，积攒的存储大小是常量，是已知可控的。更方便使用者对训练中的存储消耗和通讯时间进行预估。</li>
</ul>
<p><strong>M<del>D</del>:Memory Defragmentation</strong></p>
<p>在前文提过，设置机制，对碎片化的存储空间进行重新整合，整出连续的存储空间。防止出现总存储足够，但连续存储不够而引起的存储请求fail。</p>
<p><strong>2.3.6.ZeRO-Offload与ZeRO-Infinity</strong></p>
<p>最后，简单介绍一下ZeRO-Offload。它的核心思想是：<strong>显存不够，内存来凑</strong>。如果我把要存储的大头卸载(offload)到CPU上，而把计算部分放到GPU上，<strong>这样比起跨机，是不是能既降显存，也能减少一些通讯压力呢</strong>？<br>ZeRO-Offload的做法是：</p>
<ul>
<li><strong>forward和backward计算量高</strong>，因此和它们相关的部分，例如参数W（fp16），activation，就全放入GPU。</li>
<li><strong>update的部分计算量低</strong>，因此和它相关的部分，全部放入CPU中。例如W(fp32)，optimizer states（fp32）和gradients(fp16)等。</li>
</ul>
<p>具体切分如下图：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306251555826.webp" >
        </sapn>
      </p>
<p>ZeRO-infinity也是同理，它们在解决的事情都是：找个除GPU之外的地方，存数据。感兴趣的朋友可以深入研究，这里就不展开了。</p>
<h2 id="3-张量模型并行（TP），Megatron-LM"><a href="#3-张量模型并行（TP），Megatron-LM" class="headerlink" title="3.张量模型并行（TP），Megatron-LM"></a>3.张量模型并行（TP），Megatron-LM</h2><p>**目前基于Transformer做大模型预训练最基本的并行范式：来自NVIDIA的张量模型并行(TP)**。它的基本思想就是把模型的参数纵向切开，放到不同的GPU上进行独立计算，然后再做聚合。</p>
<p>这个老哥的行文思路：</p>
<p>全文结构如下：<br>    一、切分权重的两种方法<br>    二、MLP层<br>    三、self-attention层<br>    四、Embedding层<br>    五、Cross-entropy层<br>    六、经典并行：TP + DP (Megatron + ZeRO)<br>    七、实验效果与GPU利用率<br>    八、参考</p>
<h3 id="3-1-切分权重"><a href="#3-1-切分权重" class="headerlink" title="3.1.切分权重"></a>3.1.切分权重</h3><p>设输入数据为X，参数为W。X的维度 = (b, s, h)，W的维度 = (h, h’)。其中：</p>
<ul>
<li><code>b</code>：batch_size，表示批量大小</li>
<li><code>s</code>：sequence_length，表示输入序列的长度</li>
<li><code>h</code>：hidden_size，表示每个token向量的维度。</li>
<li><code>h&#39;</code>：参数W的hidden_size。</li>
</ul>
<p>则每次forward的过程如下：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306251701391.webp" >
        </sapn>
      </p>
<p>为画图方便，图中所绘是<code>b=1</code>时的情况。<br>假设现在W太大，导致单卡装不下。我们需要把W切开放到不同的卡上，则我们面临三个主要问题：</p>
<ul>
<li>怎么切分W。</li>
<li>切完W后，怎么做forward。</li>
<li>做完forward后，怎么做backward，进而求出梯度，更新权重。</li>
</ul>
<p>一般来说，我们可以沿着W的行（h维度），或者列（h’维度）切分W。下面我们分别介绍这两种切割办法，并说明它们是如何做forward和backward的。</p>
<p><strong>3.1.1.按行切分权重</strong></p>
<p>（1）forward</p>
<p>我们用<code>N</code>来表示GPU的数量。有几块GPU，就把W按行维度切成几份。下图展示了N=2时的切割方式：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306251707376.webp" >
        </sapn>
      </p>
<p>W按照行维度切开后，X的维度和它不对齐了，这可怎么做矩阵乘法呢？很简单，再把X“按列切开”就行了，如下图所示：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306251709750.webp" >
        </sapn>
      </p>
<p>（2）backward</p>
<p>做完forward，取得预测值Y，进而可计算出损失L，接下来就能做backward了。我们重画一下forward的过程，并在其中加入backward的部分，整体流程图如下：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306251716931.webp" >
        </sapn>
      </p>
<ul>
<li>**<code>f</code> 和 <code>g</code>**：分别表示两个算子，每个算子都包含一组forward + backward操作。forward操作已讲过，不再赘述。</li>
<li>图中的每一行，表示单独在一块GPU上计算的过程</li>
<li><strong><code>g</code> 的backward</strong>：假定现在我们要对W<del>i</del>求梯度，则可推出:</li>
</ul>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306251716536.png" >
        </sapn>
      </p>
<p>​        也就是说，只要把：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306251718065.png" >
        </sapn>
      </p>
<p>​        同时广播到两块GPU上，两块GPU就可以<strong>独立计算</strong>各自权重的梯度了。</p>
<ul>
<li><p><strong><code>f</code> 的backward</strong>：在上图中，我们只画了模型其中一层的计算过程。当模型存在多层时，梯度要从上一层向下一层传播。比如图中，梯度要先传播到X，然后才能往下一层继续传递。这就是<code>f</code> 的backward的作用。这里也易推出:</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306251718092.png" >
        </sapn>
      </p>
</li>
</ul>

        <!-- 分类文章 -->
        
      </div>
      <div class="post-content-inner-space">
        
          <div class="space-toc-main animate__animated  animate__fadeInUp">
            <ol class="space-toc"><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AFMegatron-LM%EF%BC%9F"><span class="space-toc-text">1.什么是Megatron-LM？</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#2-%E5%89%8D%E7%BC%80%E7%9F%A5%E8%AF%86"><span class="space-toc-text">2.前缀知识</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#3-%E5%BC%A0%E9%87%8F%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C%EF%BC%88TP%EF%BC%89%EF%BC%8CMegatron-LM"><span class="space-toc-text">3.张量模型并行（TP），Megatron-LM</span></a></li></ol>
           </div>
        
      </div>
   </div>
    <!-- 评论 -->
    
  </div>
</article>
  </div>
</div>



<!-- 如果是home模式的话，不在首页就显示footer，如果不是home模式的话 所有都显示footer -->

  <div class="footer-outer animate__animated  animate__fadeInUp">
    <div class="footer-inner">
    <div class="footer-text">
    <p>Power by <a target="_blank" rel="noopener" href="http://hexo.io/">Hexo</a> Theme by <a target="_blank" rel="noopener" href="https://github.com/FuShaoLei/hexo-theme-white">White</a></p>

    </div>
    <div class="footer-contact">
    <ul class="footer-ul">
        
        <li class="footer-li">
            <a href="https://github.com/YiyangZhou" target="_blank">
                <i class="ri-github-line"></i>
            </a>
        </li>
        
        <li class="footer-li">
            <a href="mailto:zhouyiyangailab@gmail.com" target="_blank">
                <i class="ri-mail-line"></i>
            </a>
        </li>
        
        <li class="footer-li">
            <a href="https://scholar.google.com/citations?user=6KltFMAAAAAJ&amp;hl=zh-CN" target="_blank">
                <i class="ri-google-fill"></i>
            </a>
        </li>
        
    </ul>
    </div>
    </div>
</div>





<script src="/js/white.js"></script>



</body>
</html>
