


<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>  Megatron-LM |    Yiyang Ai Lab.</title>
  <meta name="description" content="Personal website. Recording some creative things">
  <!-- 标签页图标 -->
  
  <link rel="shortcut icon" href="https://github.com/YiyangZhou/imageBeds/blob/main/imgs/collage-line.png" type="image/x-icon">
  

  <!-- 图标库 -->
  <link href="https://cdn.jsdelivr.net/npm/remixicon@2.2.0/fonts/remixicon.css" rel="stylesheet">
  <!-- 动画库 -->
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fushaolei/cdn-white@1.0/css/animate.css"/>
  
  <!-- css文件 -->
  
<link rel="stylesheet" href="/css/white.css">

  <!-- 代码高亮 -->
  
<meta name="generator" content="Hexo 5.4.2"></head>


<body>

<div class="menu-outer">
    <div class="menu-inner">
      <div class="menu-site-name  animate__animated  animate__fadeInUp">
        <a href="/">
          Yiyang Ai Lab.
        </a>
        
      </div>
      <div class="menu-group">
        <ul class="menu-ul">
        
          <a href="/" class="nav-link">
            <li class="menu-li  animate__animated  animate__fadeInUp">
              HOME
            </li>
          </a>
        
          <a href="/archives" class="nav-link">
            <li class="menu-li  animate__animated  animate__fadeInUp">
              BLOG
            </li>
          </a>
        
        
        
          <li class="menu-li animate__animated  animate__fadeInUp" id="mobile-menu">
            <i class="ri-menu-line"></i>
          </li>
        
        </ul>

      </div>

    </div>
</div>
<div id="mobile-main" class="animate__animated  animate__fadeIn">
  <div class="mobile-menu-inner">
    <div class="mobile-menu-site-name animate__animated  animate__fadeInUp">
      <a href="/">
        Yiyang Ai Lab.
      </a>
    </div>
    <div class="mobile-menu-group" id="mobile-close">
      <i class="ri-close-line"></i>
    </div>

  </div>

  <div class="mobile-menu-div">
  
    <a href="/" class="mobile-nav-link">
      <div class="mobile-menu-child animate__animated  animate__fadeInUp">
        <span>HOME</span>
      </div>
    </a>
  
    <a href="/archives" class="mobile-nav-link">
      <div class="mobile-menu-child animate__animated  animate__fadeInUp">
        <span>BLOG</span>
      </div>
    </a>
  
  
  </div>


</div>

<div class="body-outer">
  <div class="body-inner">
    
<article class="post-inner">
  <div class="post-content-outer">
    <div class="post-intro">
      <div class="post-title animate__animated  animate__fadeInUp">Megatron-LM</div>
      <div class="meta-intro animate__animated  animate__fadeInUp">Jun 06 2023</div>
      
    </div>
    <div class="post-content-inner">
      <div class="post-content-inner-space">

      </div>
      <div class="post-content-main animate__animated  animate__fadeInUp">
        <!-- top型目录 -->
        
        <h1 id="Megatron-LM"><a href="#Megatron-LM" class="headerlink" title="Megatron-LM"></a>Megatron-LM</h1><p>整个知识摘抄于，摘抄一遍记忆深刻，这个博客写的是真好：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/613196255">https://zhuanlan.zhihu.com/p/613196255</a></p>
<h2 id="1-什么是Megatron-LM？"><a href="#1-什么是Megatron-LM？" class="headerlink" title="1.什么是Megatron-LM？"></a>1.什么是Megatron-LM？</h2><p>Megatron-LM是一个基于pytorch的框架，专门用于训练基于transformer架构的大语言模型。</p>
<p>他通过以下几种方式实现了高效的大语言模型训练：</p>
<p>1.模型并行：将模型参数分散在多个GPU上，减少单个GPU的内存占用。</p>
<p>2.数据并行：将数据批次分散在多个GPU上，增加训练吞吐量。</p>
<p>3.混合精度：使用16位浮点数代替32位浮点数，减少内存和带宽需求，提高计算速度。</p>
<p>4.梯度累积：在多个数据批次上累计梯度，然后再更新参数，降低通讯开销。</p>
<p>5.Megatron-LM还可以与其他框架如DeepSpeed结合，实现更高级的并行：ZeRO分片和管道并行。</p>
<h2 id="2-前缀知识"><a href="#2-前缀知识" class="headerlink" title="2.前缀知识"></a>2.前缀知识</h2><h3 id="2-1-流水线并行（Pipeline-Parallelism）"><a href="#2-1-流水线并行（Pipeline-Parallelism）" class="headerlink" title="2.1.流水线并行（Pipeline Parallelism）"></a>2.1.流水线并行（Pipeline Parallelism）</h3><p>本篇文章将探索流水线并行，经典的流水线并行范式有Google推出的Gpipe，和微软推出的PipeDream。两者的推出时间都在2019年左右，大体设计框架一致。主要差别为：在梯度更新上，Gpipe是同步的，PipeDream是异步的。异步方法更进一步降低了GPU的空转时间比。虽然PipeDream设计更精妙些，但是Gpipe因为其“够用”和浅显易懂，更受大众欢迎（torch的pp接口就基于Gpipe）。因此本文以Gpipe作为流水线并行的范例进行介绍。</p>
<p>本文主要内容包括：</p>
<p>1、优化目标<br>2、模型并行<br>3、流水线并行</p>
<ul>
<li>切分micro-batch</li>
<li>Re-materialization (active checkpoint)</li>
</ul>
<p><strong>1、优化目标</strong></p>
<p>难点：</p>
<ul>
<li>训练更大的模型时，每块GPU里不仅要存模型参数，还要存中间结果（用来做Backward）。而更大的模型意味着需要更多的训练数据，进一步提高了中间结果的大小。加重了每块GPU的内存压力。</li>
<li>网络通讯开销。数据在卡之间进行传输，是需要通讯时间的。不做设计的话，这个通讯时间可能会抹平多卡本身带来的训练速度提升。</li>
</ul>
<p><strong>2.模型并行</strong></p>
<p>最简单最直接的方法是将模型分层，因为网络有很多层，将不同的层放到不同的GPU上。</p>
<p>但是这样的话（每行表示一个gpu，每一列代表一个时间步）：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230614115218668.png" >
        </sapn>
      </p>
<p>这张图的含义是：我在GPU0上做完一次forward，然后将GPU0上最后一层的输入传给GPU1，继续做forward，直到四块GPU都做完forward后，我再依次做backward。等把四块GPU上的backward全部做完后，最后一个时刻我统一更新每一层的梯度。</p>
<p>这种方法会有缺陷：</p>
<p>==GPU利用率低==</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230614115610343.png" >
        </sapn>
      </p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230614115631260.png" >
        </sapn>
      </p>
<p>==中间结果占用大量内存==</p>
<p>因为每层需要存储每一层的结果，这样会导致每个gpu都会占用额外的一部分内存来存储这一结果，如果每一层的宽度太大（也就是模型变大）会导致这一个额外的存储太大平滑掉GPU增多带来的收益。</p>
<p><strong>3.流水线并行</strong></p>
<p>Gpipe提出了流水线并行，来解决上述两个问题</p>
<p>3.1.切分micro-batch</p>
<p>流水线并行的核心思想是：<strong>在模型并行的基础上，进一步引入数据并行的办法，即把原先的数据再划分成若干个batch，送入GPU进行训练</strong>。未划分前的数据，叫<strong>mini-batch</strong>。在mini-batch上再划分的数据，叫<strong>micro-batch</strong>。</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230614131618033.png" >
        </sapn>
      </p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230614131709553.png" >
        </sapn>
      </p>
<p>3.2.re-materialization（active checkpoint）</p>
<p>解决了GPU的空置问题，提升了GPU计算的整体效率。接下来，就要解决GPU的内存问题了。<br>前文说过，随着模型的增加，每块GPU中存储的中间结果也会越大。对此，Gpipe采用了一种非常简单粗暴但有效的办法：<strong>用时间换空间，在论文里，这种方法被命名为re-materalization，后人也称其为active checkpoint</strong>。<br>具体来说，就是<strong>几乎不存中间结果，等到backward的时候，再重新算一遍forward</strong>，图例如下：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230614132106246.png" >
        </sapn>
      </p>
<p>每块GPU上，我们只保存来自上一块的最后一层输入z，其余的中间结果我们算完就废。等到backward的时候再由保存下来的z重新进行forward来算出。<br>现在我们来计算每块GPU峰值时刻的内存：<br><strong>每块GPU峰值时刻存储大小 = 每块GPU上的输入数据大小 + 每块GPU在forward过程中的中间结果大小</strong></p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230614132131221.png" >
        </sapn>
      </p>
<p>最后，再提一点，在micro-batch的划分下，我们在计算<strong>Batch Normalization</strong>时会有影响。Gpipe的方法是，在训练时计算和运用的是micro-batch里的均值和方差，但同时持续追踪全部mini-batch的移动平均和方差，以便在测试阶段进行使用。Layer Normalization则不受影响。</p>
<h3 id="2-2-数据并行（DP-DDP）"><a href="#2-2-数据并行（DP-DDP）" class="headerlink" title="2.2.数据并行（DP,DDP）"></a>2.2.数据并行（DP,DDP）</h3><p>实际应用中，流水线并行并不特别流行，主要原因是模型能否均匀切割，影响了整体计算效率，这就需要算法工程师做手调。<strong>因此，今天我们来介绍一种应用最广泛，最易于理解的并行范式：数据并行。</strong></p>
<p>数据并行的核心思想是：<strong>在各个GPU上都拷贝一份完整模型，各自吃一份数据，算一份梯度，最后对梯度进行累加来更新整体模型</strong>。理念不复杂，但到了大模型场景，<strong>巨大的存储和GPU间的通讯量，</strong>就是系统设计要考虑的重点了。在本文中，我们将递进介绍三种主流数据并行的实现方式：</p>
<ul>
<li><strong>DP（Data Parallelism）</strong>：最早的数据并行模式，一般采用参数服务器(Parameters Server)这一编程框架。实际中多用于单机多卡</li>
<li><strong>DDP（Distributed Data Parallelism）</strong>：分布式数据并行，采用Ring AllReduce的通讯方式，实际中多用于多机场景</li>
<li><strong>ZeRO：</strong>零冗余优化器。由微软推出并应用于其DeepSpeed框架中。严格来讲ZeRO采用数据并行+张量并行的方式，旨在降低存储。</li>
</ul>
<p><strong>2.2.1.DP</strong></p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230614133648185.png" >
        </sapn>
      </p>
<p>一个经典数据并行的过程如下：</p>
<ul>
<li>若干块<strong>计算GPU</strong>，如图中GPU0~GPU2；1块<strong>梯度收集GPU</strong>，如图中AllReduce操作所在GPU。</li>
<li>在每块计算GPU上都拷贝一份完整的模型参数。</li>
<li>把一份数据X（例如一个batch）均匀分给不同的计算GPU。</li>
<li>每块计算GPU做一轮FWD和BWD后，算得一份梯度G。</li>
<li>每块计算GPU将自己的梯度<strong>push</strong>给梯度收集GPU，做聚合操作。这里的聚合操作一般指<strong>梯度累加</strong>。当然也支持用户自定义。</li>
<li>梯度收集GPU聚合完毕后，计算GPU从它那<strong>pull</strong>下完整的梯度结果，用于更新模型参数W。更新完毕后，计算GPU上的模型参数依然保持一致。</li>
<li><strong>聚合再下发梯度的操作，称为AllReduce</strong>。</li>
</ul>
<p>前文说过，实现DP的一种经典编程框架叫“参数服务器”，在这个框架里，<strong>计算GPU称为Worker</strong>，<strong>梯度聚合GPU称为Server。</strong>在实际应用中，为了尽量减少通讯量，一般可选择一个Worker同时作为Server。比如可把梯度全发到GPU0上做聚合。需要再额外说明几点：</p>
<ul>
<li>1个Worker或者Server下可以不止1块GPU。</li>
<li>Server可以只做梯度聚合，也可以梯度聚合+全量参数更新一起做</li>
</ul>
<p>==存在的问题==</p>
<p>DP的框架理解起来不难，但实战中确有两个主要问题：</p>
<ul>
<li><p><strong>存储开销大</strong>。每块GPU上都存了一份完整的模型，造成冗余。关于这一点的优化，我们将在后文ZeRO部分做讲解。</p>
</li>
<li><p><strong>通讯开销大</strong>。Server需要和每一个Worker进行梯度传输。当Server和Worker不在一台机器上时，Server的带宽将会成为整个系统的计算效率瓶颈。</p>
</li>
<li><p><strong>worker摸鱼</strong>。如果集中push完然后等待pull操作，那么所有的worker都在摸鱼。</p>
</li>
</ul>
<p>为了解决第三个问题提出了<strong>梯度异步更新</strong></p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230614134335872.png" >
        </sapn>
      </p>
<p>上图刻画了在<strong>梯度异步更新</strong>的场景下，某个Worker的计算顺序为：</p>
<ul>
<li>在第10轮计算中，该Worker正常计算梯度，并向Server发送push&amp;pull梯度请求。</li>
<li>但是，该Worker并不会实际等到把聚合梯度拿回来，更新完参数W后再做计算。而是直接拿旧的W，吃新的数据，继续第11轮的计算。<strong>这样就保证在通讯的时间里，Worker也在马不停蹄做计算，提升计算通讯比。</strong></li>
<li>当然，异步也不能太过份。只计算梯度，不更新权重，那模型就无法收敛。图中刻画的是<strong>延迟为1</strong>的异步更新，也就是在开始第12轮对的计算时，必须保证W已经用第10、11轮的梯度做完2次更新了。</li>
</ul>
<p>参数服务器的框架下，延迟的步数也可以由用户自己决定，下图分别刻划了几种延迟情况：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/image-20230614134627712.png" >
        </sapn>
      </p>
<p>总结一下，<strong>异步很香，但对一个Worker来说，只是等于W不变，batch的数量增加了而已，在SGD下，会减慢模型的整体收敛速度</strong>。异步的整体思想是，比起让Worker闲着，倒不如让它多吃点数据，虽然反馈延迟了，但只要它在干活在学习就行。<br>batch就像活，异步就像画出去的饼，且往往不指定延迟步数，每个Worker干越来越多的活，但模型却没收敛。</p>
<p><strong>2.2.2.DDP</strong></p>
<p>受通讯负载不均的影响，<strong>DP一般用于单机多卡场景</strong>。因此，DDP作为一种更通用的解决方案出现了，既能多机，也能单机。<strong>DDP首先要解决的就是通讯问题：将Server上的通讯压力均衡转到各个Worker上。实现这一点后，可以进一步去Server，留Worker。</strong><br>前文我们说过，聚合梯度 + 下发梯度这一轮操作，称为AllReduce。<strong>接下来我们介绍目前最通用的AllReduce方法：Ring-AllReduce</strong>。它由百度最先提出，非常有效地解决了数据并行中通讯负载不均的问题，使得DDP得以实现。</p>
<p>AllReduc类似于：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306201130885.png" >
        </sapn>
      </p>
<p>但是对于Ring-AllReduce则分为两大步骤实现该目标：<strong>Reduce-Scatter</strong>和<strong>All-Gather。</strong></p>
<p><strong>Reduce-Scatter</strong><br>定义网络拓扑关系，使得每个GPU只和其相邻的两块GPU通讯。每次发送对应位置的数据进行<strong>累加</strong>。每一次累加更新都形成一个拓扑环，因此被称为Ring。看到这觉得困惑不要紧，步骤类似于这样，这里又想夸作者了，写得真好：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306201132889.png" >
        </sapn>
      </p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306201133370.png" >
        </sapn>
      </p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306201133406.png" >
        </sapn>
      </p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306201134140.png" >
        </sapn>
      </p>
<p><strong>3次</strong>更新之后，每块GPU上都有一块数据拥有了对应位置完整的聚合（图中红色）。此时，Reduce-Scatter阶段结束。进入All-Gather阶段。目标是把红色块的数据广播到其余GPU对应的位置上。</p>
<p><strong>All-Gather</strong><br>如名字里Gather所述的一样，这操作里依然按照“相邻GPU对应位置进行通讯”的原则，但对应位置数据不再做相加，而是直接替换。All-Gather以红色块作为起点。</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306201135723.png" >
        </sapn>
      </p>
<p>以此类推，同样经过<strong>3轮迭代后</strong>，使得每块GPU上都汇总到了完整的数据，变成如下形式：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306201135189.png" >
        </sapn>
      </p>
<p><strong>Ring-AllReduce的通讯量分析</strong></p>
<p>假设model的参数量是&amp;，GPU的个数为N，梯度的大小总量为w，那么每个GPU的梯度大小为W/N（只算其通讯量）：</p>
<ul>
<li>Reduce-Scatter阶段的通讯量为：（N - 1）（&amp;/N）</li>
<li>All-Gather阶段的通讯量为：（N - 1）(&amp;/N)</li>
</ul>
<p>其中的N-1来源于上述图的推导。所以单卡的通讯量为2（N - 1）(&amp;/N) –&gt; 2&amp;，总的通讯量为2N&amp;。</p>
<p>这和之前的All-Reduce的通讯量一样的，但是其中的Server只用来当搬运工，当越来越多的GPU分布到较远的机器上时，DP的通讯时间是很大的。</p>
<p><strong>总结</strong></p>
<p>1、在DP中，每个GPU上都拷贝一份完整的模型，每个GPU上处理batch的一部分数据，所有GPU算出来的梯度进行累加后，再传回各GPU用于更新参数<br>2、DP多采用参数服务器这一编程框架，一般由若个计算Worker和1个梯度聚合Server组成。Server与每个Worker通讯，Worker间并不通讯。因此Server承担了系统所有的通讯压力。基于此DP常用于单机多卡场景。<br>3、异步梯度更新是提升计算通讯比的一种方法，延迟更新的步数大小决定了模型的收敛速度。<br>4、Ring-AllReduce通过定义网络环拓扑的方式，将通讯压力均衡地分到每个GPU上，使得跨机器的数据并行（DDP）得以高效实现。<br>5、DP和DDP的总通讯量相同，但因负载不均的原因，DP需要耗费更多的时间搬运数据。</p>
<h3 id="2-3-ZeRO（零冗余优化）"><a href="#2-3-ZeRO（零冗余优化）" class="headerlink" title="2.3.ZeRO（零冗余优化）"></a>2.3.ZeRO（零冗余优化）</h3><p>之前两种方法DP和DDP，DP存在负载不均的问题，但是他俩还存在一个共同问题就是显存开销问题，数据并行中，每个GPU上都复制了一份完整模型，当模型变大时，很容易打爆GPU的显存。</p>
<p>下面介绍一下微软开发的<strong>ZeRO（零冗余优化）</strong>，它是DeepSpeed这一分布式训练框架的核心，被用来解决大模型训练中的显存开销问题。他的主旨思想就是<strong>通讯换显存</strong>。</p>
<p><strong>2.3.1.训练过程中显卡需要存储的东西</strong></p>
<p>存储主要分为两大块：Model States和Residual States<br><strong>Model States</strong>指和模型本身息息相关的，必须存储的内容，具体包括：</p>
<ul>
<li><strong>optimizer states</strong>：Adam优化算法中的momentum和variance</li>
<li><strong>gradients</strong>：模型梯度</li>
<li><strong>parameters</strong>：模型参数W</li>
</ul>
<p><strong>Residual States</strong>指并非模型必须的，但在训练过程中会额外产生的内容，具体包括：</p>
<ul>
<li><strong>activation</strong>：激活值。在流水线并行中我们曾详细介绍过。在backward过程中使用链式法则计算梯度时会用到。有了它算梯度会更快，但它不是必须存储的，因为可以通过重新做Forward来算它。</li>
<li><strong>temporary buffers:</strong> 临时存储。例如把梯度发送到某块GPU上做加总聚合时产生的存储。</li>
<li><strong>unusable fragment memory</strong>：碎片化的存储空间。虽然总存储空间是够的，但是如果取不到连续的存储空间，相关的请求也会被fail掉。对这类空间浪费可以通过内存整理来解决。</li>
</ul>
<p><strong>2.3.2.精度混合训练</strong></p>
<p>对于模型，我们肯定希望其参数越精准越好，也即我们用<strong>fp32（单精度浮点数，存储占4byte）</strong>来表示参数W。但是在forward和backward的过程中，fp32的计算开销也是庞大的。那么能否在计算的过程中，引入<strong>fp16或bf16（半精度浮点数，存储占2byte）</strong>，来减轻计算压力呢？于是，混合精度训练就产生了，它的步骤如下图：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306201201271.png" >
        </sapn>
      </p>
<ul>
<li>存储一份fp32的parameter，momentum和variance（统称model states）</li>
<li>在forward开始之前，额外开辟一块存储空间，将fp32 parameter减半到fp16 parameter。</li>
<li>正常做forward和backward，在此之间产生的activation和gradients，都用fp16进行存储。</li>
<li>用fp16 gradients去更新fp32下的model states。</li>
<li>当模型收敛后，fp32的parameter就是最终的参数输出。</li>
</ul>
<p><strong>2.3.3.显存大小</strong></p>
<p>现在，我们可以来计算模型在训练时需要的存储大小了，假设模型的参数W大小是 Φ ，<strong>以byte为单位</strong>，存储如下：</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="https://raw.githubusercontent.com/YiyangZhou/imageBeds/main/imgs/202306201206441.png" >
        </sapn>
      </p>
<p>因为采用了Adam优化，所以才会出现momentum和variance，当然你也可以选择别的优化办法。因此这里为了更通用些，记模型必存的数据大小为 KΦ 。因此最终内存开销为： 2Φ+2Φ+KΦ</p>
<p>另外，这里暂时不将activation纳入统计范围，原因是：</p>
<ul>
<li>activation不仅与模型参数相关，还与batch size相关。</li>
<li>activation的存储不是必须的。存储activation只是为了在用链式法则做backward的过程中，计算梯度更快一些。但你永远可以通过只保留最初的输入X，重新做forward来得到每一层的activation（虽然实际中并不会这么极端）。</li>
<li>因为activation的这种灵活性，纳入它后不方便衡量系统性能随模型增大的真实变动情况。因此在这里不考虑它，在后面会单开一块说明对activation的优化。</li>
</ul>
<p><strong>2.3.4.ZeRO-DP</strong></p>
<p>知道了什么东西会占存储，以及它们占了多大的存储之后，我们就可以来谈如何优化存储了。注意到，在整个训练中，有很多states并不会每时每刻都用到，举例来说；</p>
<ul>
<li>Adam优化下的optimizer states只在最终做update时才用到</li>
<li>数据并行中，gradients只在最后做AllReduce和updates时才用到</li>
<li>参数W只在做forward和backward的那一刻才用到</li>
<li>诸如此类</li>
</ul>

        <!-- 分类文章 -->
        
      </div>
      <div class="post-content-inner-space">
        
          <div class="space-toc-main animate__animated  animate__fadeInUp">
            <ol class="space-toc"><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AFMegatron-LM%EF%BC%9F"><span class="space-toc-text">1.什么是Megatron-LM？</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#2-%E5%89%8D%E7%BC%80%E7%9F%A5%E8%AF%86"><span class="space-toc-text">2.前缀知识</span></a></li></ol>
           </div>
        
      </div>
   </div>
    <!-- 评论 -->
    
  </div>
</article>
  </div>
</div>



<!-- 如果是home模式的话，不在首页就显示footer，如果不是home模式的话 所有都显示footer -->

  <div class="footer-outer animate__animated  animate__fadeInUp">
    <div class="footer-inner">
    <div class="footer-text">
    <p>Power by <a target="_blank" rel="noopener" href="http://hexo.io/">Hexo</a> Theme by <a target="_blank" rel="noopener" href="https://github.com/FuShaoLei/hexo-theme-white">White</a></p>

    </div>
    <div class="footer-contact">
    <ul class="footer-ul">
        
        <li class="footer-li">
            <a href="https://github.com/YiyangZhou" target="_blank">
                <i class="ri-github-line"></i>
            </a>
        </li>
        
        <li class="footer-li">
            <a href="mailto:zhouyiyangailab@gmail.com" target="_blank">
                <i class="ri-mail-line"></i>
            </a>
        </li>
        
        <li class="footer-li">
            <a href="https://scholar.google.com/citations?user=6KltFMAAAAAJ&amp;hl=zh-CN" target="_blank">
                <i class="ri-google-fill"></i>
            </a>
        </li>
        
    </ul>
    </div>
    </div>
</div>





<script src="/js/white.js"></script>



</body>
</html>
